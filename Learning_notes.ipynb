{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTモデルの事前学習中に思わぬアクシデントに何回も遭遇したため、<br>\n",
    "大規模な学習を行う人のために学習上の注意のメモ書きを残しておこうと思いました。<br>\n",
    "このファイルの内容は[PyTorch Performance Tuning Guide - Szymon Migacz, NVIDIA](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf)というNVIDIAの公演スライドと自身の学習経験を元に制作されています。\n",
    "<br>\n",
    "内容\n",
    "- 学習中にGPUのメモリが足りなくなった時の対処法\n",
    "- 自動混合精度機能を用いた学習の注意点\n",
    "- その他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習中にGPUのメモリが足りなくなった時の対処法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. メモリをbatch毎に解放する。<br>\n",
    "batch毎にbatch内部で出てきた変数は消去した方がよい。Pytorch内部の計算グラフが代入によって残ってしまうからか、<br>\n",
    "Pythonそもそもの仕様なのかはわからないが(詳しい人教えてください)、計算に使用してbatch内部で二度と使わない変数は<br>\n",
    "削除しないと使えるメモリ領域が減っている。<br>\n",
    "また、ipynb特有の使用で標準出力されてしまったものはキャッシュされているようである。<br>\n",
    "学習の際は.ipynbでやるよりも.pyで実行した方が良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモリ解放のやり方は以下の通りである。GPUを使用しているときはGPU上の変数のメモリ領域の解放も忘れてはいけない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc #メモリ解放を行うライブラリ\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.randn(1000,1000).to(device)\n",
    "del x\n",
    "gc.collect() #メモリを解放\n",
    "torch.cuda.empty_cache() #GPUのメモリを解放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (CUDAが使える環境限定)自動混合精度機能を使う。<br>\n",
    "モデルがメモリ不足を引き起こす理由として次に考えられるのは変数がfloat32だからである。<br>\n",
    "Pytorchではデフォルトの計算でfloat32を使うように指定されているが、深層学習の学習によってはfloat32レベルの精度はいらない可能性がある。<br>\n",
    "これをfloat16やbfloat16と同時に使用することでメモリ使用量を抑え、さらに計算時間も短縮できる機能がCUDAには盛り込まれている。<br>\n",
    "Pytorchの公式ドキュメントがこれに関して詳しく書かれているので参考にしてほしい->[torch.cuda.amp](https://pytorch.org/docs/stable/notes/amp_examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雛形は以下の通りです。(以下のコードは実行できません)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下の自動混合精度機能を用いた計算における追加されたPytorchの機能は以下の二つです。\n",
    "- torch.cuda.amp.GradScaler\n",
    "- torch.cuda.amp.autocast<br>\n",
    "<br>\n",
    "torch.cuda.amp.GradScalerはfloat32からfloat16やbfloat16にキャストすることで起こるアンダーフローを防止するために損失に大きな数をかけて<br>\n",
    "勾配を大きくしてこれを防ぎます。損失にかける値のデフォルト値は$2^{16}$となります。<br>\n",
    "これを実現している箇所がscaler.scale(loss).backward()のところです。<br>\n",
    "逆伝播が終わってから元の勾配の大きさに戻してパラメーターを更新している箇所がscaler.step(optimizer)になります。<br>\n",
    "scaler.update()では次のパラメーター更新のためのスケールの更新を行なっています。<br>\n",
    "<br>\n",
    "自動混合精度計算を実現している箇所はwith torch.cuda.amp.autocastから始まるブロックの部分になります。<br>\n",
    "ここの内部では型をキャストしても問題ない計算のみ型をキャストして計算を行なっています。<br>\n",
    "もちろん深層学習で非常に多く使われる行列計算はキャストの対象になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr)\n",
    "scaler = torch.cuda.amp.GradScaler(init_scale=2**16, enabled=True)\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. batch_sizeを増やすとGPUに変数が乗らない場合は勾配累積を使う<br>\n",
    "これはbatch毎に勾配を計算してパラメーターを計算するのでは無く、batchの計算では勾配を溜めておいて、最後にパラメーターを更新するというものです。<br>\n",
    "例えばbatch_sizeが5のデータを6回繰り返せばbatch_sizeが30のデータの勾配を計算することになります。<br>\n",
    "Pytorchの勾配の仕様として、zero_gradにするまで計算した勾配は加算されていくため、その仕様を利用しましょう。<br>\n",
    "これにより、自然言語などのタスクでは特定の文章に過剰に適合するなどの問題が起こらなくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装例としては以下のとおりです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr)\n",
    "scaler = torch.cuda.amp.GradScaler(init_scale=2**16, enabled=True)\n",
    "for epoch in epochs:\n",
    "    optimizer.zero_grad()\n",
    "    for input, target in data:\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "            scaler.scale(loss / len(data)).backward() #勾配は計算しておくだけ、平均しておかないとおかないと損失は大きくなるのでdataのサイズで割る\n",
    "    scaler.step(optimizer) #最後にパラメーターを更新する。\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 量子化を行う。<br>\n",
    "モデルのパラメーターfloat32をint8に変換するtorch.qintという型があります。これによりモデルのサイズを減らすことができます。<br>\n",
    "$\\to$[公式ドキュメント](https://pytorch.org/docs/stable/quantization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 上4つを試したけど学習がうまくいかない場合<br>\n",
    "もうどうしようもないので泣きながらモデルの縮小かGPUを増やしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動混合精度機能を用いた学習の注意点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自動混合精度計算には思わぬ落とし穴があり、それで何時間、何日も学習を無駄にする場合があります。実際の経験からそれらの対処法について解説したいと思います。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 絶対にfloat16でキャストをするな。bfloat16でキャストをしろ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非常にシンプルですがこれだけです。理由を説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16:  tensor(0., dtype=torch.float16)\n",
      "bfloat16:  tensor(1.0012e-08, dtype=torch.bfloat16)\n",
      "float32:  tensor(1.0000e-08)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1e-8)\n",
    "print(\"float16: \", x.to(torch.float16))\n",
    "print(\"bfloat16: \", x.to(torch.bfloat16))\n",
    "print(\"float32: \", x.to(torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のコードから、float16は1e-8以下の数字は0にキャストしてしまうことがわかります。<br>\n",
    "これはゼロ割を含みうる計算の結果がNaNとなるおそれがあり、結果に大きく作用します。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 損失がでかすぎてもNaNになるので、torch.cuda.amp.GradScalerの初期化変数のinit_scaleは小さめにとること"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期値は$2^{16}$と非常に大きいです。cos schedulerなどを使っている場合、学習率が大きくなってくると損失が大きくなる恐れがあるので小さめに設定しておきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一度NaNを含む計算結果で勾配を計算してしまうと全てのパラメーターがNaNとなり、学習のやり直しが発生します。<br>\n",
    "こうならないためにも学習中のコードでは以下のコードを書いておき、lossがNaNの時に計算をやめるようにしておくことをおすすめします。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.isnan(loss.detach()):\n",
    "    print(\"Loss is NaN.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習の高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. モデルをコンパイルする<br>\n",
    "モデルをtorch.compileでコンパイルすることで計算の効率化、最適化を行ってくれるものになります。使わない手はないでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. torch.backendsをいじる<br>\n",
    "- torch.backends.cudnn.benchmark<br>torch.backends.cudnn.benchmarkをTrueにすることでネットワークの構成に対して最適なアルゴリズムを見つけて計算を行うため、計算が早くなります。<br>(しかし、再現性はなくなります。)\n",
    "- torch.backends.cudnn.allow_tf32<br>\n",
    "torch.backends.cudnn.allow_tf32をTrueにすることで対応しているGPUならばTensorFloat32コアを使用して計算を行います。<br>\n",
    "他にもtorch.backendsには計算の最適化を行う機能があるので調べてみると良いです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
