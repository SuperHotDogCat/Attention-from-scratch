{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このファイルではGPTモデルの実装を目標とする。このファイルの立ち位置は同リポジトリに含まれている<br>\n",
    "・pytorch_command.ipynb<br>\n",
    "・attention_from_scratch.ipynb<br>\n",
    "の次に読むことを想定されている。pytorchの下位APIは上２つの.ipynbで散々練習したので、上位APIを解禁して最新モデルを組むことを目指す<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchに明るい人はこのipynbファイルから読んでも良い、pytorchに明るくない人は上２つのipynbファイルを公式ドキュメントとにらめっこして読むことをおすすめする。<br>\n",
    "内容がわかりにくいと感じた人はGithub上もしくはXで作者に質問/書き換え要請を投げることができる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 本題\n",
    "実装においては[GPT from scratch](https://jaketae.github.io/study/gpt/)やKarpathyの[minGPT](https://github.com/karpathy/minGPT)を参考にする。<br>\n",
    "実装に関してはこのipynbを見なくとも紹介した2つのサイトを見れば良い。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装に入る前に簡単なGPTの実装方針についての概要について解説する。今回実装するのは初代GPTである。<br>\n",
    "初代GPTは以下のような構造をしている<br>\n",
    "- GPTはTransformerのDecoderのみを用いたモデルである。\n",
    "- Embedding_dimは768, MultiheadAttentionのHead数は12, TransformerDecoderブロック数は12である。\n",
    "- FFN層の活性化関数はGELUである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練時の注意事項も記載しておく\n",
    "- OptimizerはAdam, 学習率の最大値は2.5e-4, 2000iteratonで最大値に達し、<br>その後はCOSINEスケジューラーによってスケジューリングを行う。attention_from_scratch.ipynbで紹介したwarm_upの改善版のようなものである。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA環境が壊れていないことを祈りながら確認-> True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "print(\"CUDA環境が壊れていないことを祈りながら確認->\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは学習につかうTransformerDecoderLayerを定義する。デフォルトにtorch.nn.TransformerDecoderLayerがあるが、<br>\n",
    "カスタムレイヤーは製作したほうが早い\n",
    "下のコードはattention_from_scratch.ipynbのTransformerDecoderLayerのFFN層の活性化関数をGELUに変えただけである。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0., layer_eps=1e-05, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.multiheadselfattention = nn.MultiheadAttention(embedding_dim, num_heads,batch_first=batch_first)\n",
    "        self.dropout_selfattn = nn.Dropout(p = drop_out_rate)\n",
    "        self.layernorm_selfattn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "\n",
    "        self.multiheadattention = nn.MultiheadAttention(embedding_dim, num_heads,batch_first=batch_first) \n",
    "        self.dropout_attn = nn.Dropout(p = drop_out_rate)\n",
    "        self.layernorm_attn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, embedding_dim))#GELUに変更\n",
    "        self.layernorm_ffn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "        self.dropout_ffn = nn.Dropout(p = drop_out_rate)\n",
    "    def forward(self, src, tgt, pad_mask_self = None, mask_self=None, pad_mask = None, mask = None):\n",
    "        dtgt, _ = self.multiheadselfattention(tgt,tgt,tgt,key_padding_mask = pad_mask_self, attn_mask = mask_self)\n",
    "        dtgt = self.dropout_selfattn(dtgt)\n",
    "        tgt = self.layernorm_selfattn(tgt+dtgt)\n",
    "        dtgt, _ = self.multiheadattention(tgt, src, src, key_padding_mask = pad_mask, attn_mask = mask)\n",
    "        dtgt = self.dropout_attn(dtgt)\n",
    "        tgt = self.layernorm_attn(tgt+dtgt)\n",
    "        dtgt = self.dropout_ffn(self.ffn(tgt))\n",
    "        tgt = self.layernorm_ffn(dtgt + tgt)\n",
    "        return tgt\n",
    "    \n",
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int,T = 10000):\n",
    "        super().__init__()\n",
    "        #PE (1, max_sequence_length, embedding_dim)\n",
    "        self.pe = torch.zeros(size=(1, T, embedding_dim))\n",
    "        t = torch.arange(start = 1, end = T+1).reshape(T, 1)\n",
    "        k_odd = torch.arange(start = 1, end = embedding_dim+1, step = 2)\n",
    "        k_odd = k_odd.reshape(1, k_odd.size(0))\n",
    "        k_even = torch.arange(start = 2, end = embedding_dim+1, step = 2)\n",
    "        k_even = k_even.reshape(1,k_even.size(0))\n",
    "        phase_odd = t / T**((k_odd//2 * 2) / embedding_dim) #<- ブロードキャスト機能を用いて効率よく計算\n",
    "        phase_even = t / T**((k_even//2 * 2) / embedding_dim)\n",
    "        self.pe[0,:,0::2] = torch.sin(phase_odd)\n",
    "        self.pe[0,:,1::2] = torch.cos(phase_even)\n",
    "        self.register_buffer(\"positional_encoding_weight\", self.pe)\n",
    "        self.embedding_dim = embedding_dim\n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"入力のxのサイズx: (batch_size, sequence_length, embedding_dim)\"\"\"\n",
    "        #np.sqrt(embedding_dim)をかけることでベクトルのスケールを合わせている\n",
    "        return np.sqrt(self.embedding_dim)*x + self.pe[:,:x.size(1),:].to(x.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0.,\\\n",
    "                  layer_eps=1e-05, batch_first = False, T = 10000, N = 1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(tgt_vocab_size, embedding_dim,)\n",
    "        self.positional_encoding = positional_encoding(embedding_dim, T)\n",
    "        self.decoder = nn.ModuleList([TransformerDecoderLayer(embedding_dim, ffn_dim, num_heads, drop_out_rate,\\\n",
    "                                                               layer_eps, batch_first) for _ in range(N)])\n",
    "    def forward(self, src, tgt, pad_mask_self = None, mask_self=None, pad_mask = None, mask = None):\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(src, tgt, pad_mask_self = pad_mask_self,mask_self = mask_self, pad_mask = pad_mask, mask = mask)\n",
    "        return tgt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
