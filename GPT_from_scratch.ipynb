{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このファイルではGPTモデルの実装を目標とする。このファイルの立ち位置は同リポジトリに含まれている<br>\n",
    "・pytorch_command.ipynb<br>\n",
    "・attention_from_scratch.ipynb<br>\n",
    "の次に読むことを想定されている。pytorchの下位APIは上２つの.ipynbで散々練習したので、上位APIを解禁して最新モデルを組むことを目指す<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchに明るい人はこのipynbファイルから読んでも良い、pytorchに明るくない人は上２つのipynbファイルを公式ドキュメントとにらめっこして読むことをおすすめする。<br>\n",
    "内容がわかりにくいと感じた人はGithub上もしくはXで作者に質問/書き換え要請を投げることができる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 本題\n",
    "実装においては[GPT from scratch](https://jaketae.github.io/study/gpt/)やKarpathyの[minGPT](https://github.com/karpathy/minGPT), [nanoGPT](https://github.com/karpathy/nanoGPT)を参考にする。<br>\n",
    "実装に関してはこのipynbを見なくとも紹介した2つのサイトを見れば良い。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装に入る前に簡単なGPTの実装方針についての概要について解説する。今回実装するのは初代GPTである。<br>\n",
    "初代GPTは以下のような構造をしている<br>\n",
    "- GPTはTransformerのDecoderのみを用いたモデルである。\n",
    "- Embedding_dimは768, MultiheadAttentionのHead数は12, TransformerDecoderブロック数は12である。\n",
    "- FFN層の活性化関数はGELUである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練時の注意事項も記載しておく\n",
    "- OptimizerはAdam, 学習率の最大値は2.5e-4, 2000iteratonで最大値に達し、<br>その後はCOSINEスケジューラーによってスケジューリングを行う。attention_from_scratch.ipynbで紹介したwarm_upの改善版のようなものである。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA環境が壊れていないことを祈りながら確認-> True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "print(\"CUDA環境が壊れていないことを祈りながら確認->\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは学習につかうTransformerDecoderLayerを定義する。デフォルトにtorch.nn.TransformerDecoderLayerがあるが、<br>\n",
    "GPTにつかうDecoderLayerは\"Attention is all you need\"で紹介されているDecoder Layerとは少し異なっている。<br>\n",
    "そのため、カスタムレイヤーを定義する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考のために比較画像を用意した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT\n",
    "<img src = \"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "<img src = \"https://user-images.githubusercontent.com/57289763/160270884-e1901241-a1e6-4890-a5e8-165e87f0c4da.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではGPTのレイヤーをGPTDecoderLayerとして定義しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0., layer_eps=1e-05, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.maskedmultiheadattention = nn.MultiheadAttention(embedding_dim, num_heads,batch_first=batch_first)\n",
    "        self.dropout_selfattn = nn.Dropout(p = drop_out_rate)\n",
    "        self.layernorm_selfattn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, embedding_dim))#GELUに変更\n",
    "        self.layernorm_ffn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "        self.dropout_ffn = nn.Dropout(p = drop_out_rate)\n",
    "\n",
    "    def forward(self, x, pad_mask_self = None, mask_self=None):\n",
    "        dx, _ = self.maskedmultiheadattention(x,x,x,key_padding_mask = pad_mask_self, attn_mask = mask_self)\n",
    "\n",
    "        dx = self.dropout_selfattn(dx)\n",
    "\n",
    "        x = self.layernorm_selfattn(x+dx)\n",
    "\n",
    "        dx = self.dropout_ffn(self.ffn(x))\n",
    "\n",
    "        x = self.layernorm_ffn(x + dx)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作したGPTDecoderLayerにEmbeddingとPositional EncodingをつければGPTモデルの定義が終わる。<br>\n",
    "マスクの制作もGPTクラス内部に含める形で実装を行なう。<br>\n",
    "だがここで注意しなければならなければならないことがある。<br>\n",
    "TransformerモデルではPositional Encodingはsinとcosを用いて実装したが、<br>\n",
    "GPTではこのPositional Encodingも学習可能パラメーターとして実装を行なう。<br>\n",
    "GPTでのこれにあたる層はnn.Embedding(max_sequence_len, embedding_dim)として埋め込み層を定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0.,\\\n",
    "                  layer_eps=1e-05, batch_first = False, T = 10000, N = 1):\n",
    "        super().__init__()\n",
    "        #Tはmax_lenを表している\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,)\n",
    "        self.positional_embedding = nn.Embedding(T, embedding_dim)\n",
    "        self.decoder = nn.ModuleList([GPTDecoderLayer(embedding_dim, ffn_dim, num_heads, drop_out_rate,\\\n",
    "                                                               layer_eps, batch_first) for _ in range(N)])\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size, bias = False)\n",
    "    def forward(self, x, y,pad_mask_self = None, mask_self=None):\n",
    "        \"\"\"\n",
    "        yはxを1つだけずらしたデータである\n",
    "        x = data[a:b]なら、y = data[a+1:b+1]となる。\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        pos = torch.arange(0,x.size(1),dtype=torch.long).unsqueeze(0).to(x.device)\n",
    "        pos = self.positional_embedding(pos)\n",
    "        x = x + pos\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, pad_mask_self = pad_mask_self, mask_self = mask_self)\n",
    "        x = self.linear(x)\n",
    "        loss = F.cross_entropy(x.view(-1, x.size(-1)), y.view(-1), ignore_index=-1) \n",
    "        #ignore_index=-1はyをonehotベクトル化しないでcross_entropyを使うために使用\n",
    "        return loss\n",
    "    def create_mask(self, x: torch.tensor, x_pad: int, device: str):\n",
    "        \"\"\"\n",
    "        (batch_size, sequence_length, embedding_dim)の入力を想定\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Trueが無視される値であることに注意すること\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        #srcのマスク制作\n",
    "        padding_mask = (x == x_pad)\n",
    "        mask = torch.triu(torch.ones(size = (seq_len, seq_len))==1).transpose(0,1) #下三角行列を作る\n",
    "        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask==1.,float(0.0)).to(device)\n",
    "        return padding_mask, mask\n",
    "    \"\"\"To do generate関数を制作する\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上手く動くか試しに検証してみよう。\n",
    "入力は(batch_size, tokens) = (1, 6)のLongTensorで、create_maskでmaskを製作し、上手く動作することを確かめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.LongTensor([[2,10,20,100,512,3],[2,10,20,100,512,3],[2,10,20,100,512,3]]).to(device)\n",
    "x = x.reshape(3,6)\n",
    "embedding_size = 768\n",
    "num_heads = 12\n",
    "#KarpathyのminGPTを参考に、パラメーターを設定した。\n",
    "gpt = GPT(50257, embedding_size, embedding_size*4, num_heads, 0.1, batch_first=True, T = 1024, N = 12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4225, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask, mask = gpt.create_mask(x[0:2], 0, device)\n",
    "gpt(x[0:2],x[1:3],padding_mask,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの構造は以下の通りである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (embedding): Embedding(50257, 768)\n",
      "  (positional_embedding): Embedding(1024, 768)\n",
      "  (decoder): ModuleList(\n",
      "    (0-11): 12 x GPTDecoderLayer(\n",
      "      (maskedmultiheadattention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout_selfattn): Dropout(p=0.1, inplace=False)\n",
      "      (layernorm_selfattn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (layernorm_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout_ffn): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "気になるパラメーター数は以下のとおりである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters is  163035648\n"
     ]
    }
   ],
   "source": [
    "count_params = 0\n",
    "for params in gpt.parameters():\n",
    "    count_params += params.contiguous().view(-1).size(0)\n",
    "print(\"The number of parameters is \", count_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT1のパラメーター数は1億程度であると言われているが、実際に組んだモデルでも1億程度になっていることが確かめられた。<br>\n",
    "途方もない数字のように思えるが、全てのパラメーターがfloat32(4バイト)であることを考えて<br>簡単な計算を行なうと,\n",
    "家にある普通のGPUでもモデルのパラメーターを乗せることができるとわかる。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#念の為メモリを開放しておく\n",
    "import gc\n",
    "del gpt\n",
    "del x\n",
    "del padding_mask, mask\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でモデルの定義は終わりである。ここからはデータセットの定義に移る。<br>\n",
    "GPTモデルでは分かち書きに工夫がなされている。これはByte-Pair Encodingと言われている。<br>\n",
    "実装はやや複雑なので別ファイルで解説を行なう。ここではtiktokenライブラリのGPT2Tokenizerを利用しようと思う。<br>\n",
    "tiktokenはOpenAIから提供されているライブラリであり、huggingfaceから提供されているGPT2Tokenizerよりも<br>\n",
    "早いことが知られている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 257, 6291, 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#本当は実装したかったけどヤムナシ\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer.encode_ordinary(\"This is a sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの定義を行なう。データはopenwebtextを用いることにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "num_proc_load_dataset = 8 #並列処理をどの程度行なうか\n",
    "dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしたらpickleで保存しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.bin\",\"wb\") as p:\n",
    "    pickle.dump(dataset,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.bin\", \"rb\") as p:\n",
    "    dataset = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ以降はデータの処理のためにHugging FaceのDatasetDictに提供されている<br>\n",
    "前処理メソッドを使って前処理を行っていきます。各メソッドの解説は[HuggingFace datasets process](https://huggingface.co/docs/datasets/process#map)にあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset分割\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test') # rename the test split to val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8009762\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットをtrainとvalに分割したため、tokenizeを行います。<br>\n",
    "mapメソッドは第一引数に辞書型を返す処理関数を与えます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing the splits (num_proc=8): 100%|██████████| 8009762/8009762 [21:33<00:00, 6194.09 examples/s]\n",
      "tokenizing the splits (num_proc=8): 100%|██████████| 4007/4007 [00:01<00:00, 2138.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def process(example):\n",
    "        ids = tokenizer.encode_ordinary(example['text']) \n",
    "        ids.append(tokenizer.eot_token) #文末に<endoftext>tokenを追加\n",
    "        #\n",
    "        out = {'ids': ids, 'len': len(ids)}\n",
    "        return out\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized = split_dataset.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=num_proc_load_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ids', 'len'],\n",
       "        num_rows: 8009762\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['ids', 'len'],\n",
       "        num_rows: 4007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizeを行ったら、同じようにpickleで保存しておく。ここまでで製作したdataset.binとtokenized_dataset.binはgithub内部にもおいておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"tokenized_dataset.bin\",\"wb\") as p:\n",
    "    pickle.dump(tokenized,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"tokenized_dataset.bin\", \"rb\") as p:\n",
    "    tokenized = pickle.load(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizeが終了したデータは一つのbinファイルにまとめる。このとき、train dataだけで800万データがあるため、<br>\n",
    "メモリ上に全データが乗らない場合がある。こんなときはnumpy.memmapを使ってファイルに書き出しを行なう。<br>\n",
    "numpy.memmap(filename, dtype, mode, shape)を指定すると、指定したファイルの置き場所にファイルが生成される。<br>\n",
    "製作したnumpy.memmapファイルはnumpy.ndarrayと同じように値が書き込めるため、800万データを少しずつ書き込めばよい。<br>\n",
    "更に詳しい説明は公式ドキュメントの[numpy.memmap](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html)を参考にして欲しい。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [02:35<00:00,  6.59it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 1293.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for split, dset in tokenized.items():\n",
    "    #split: train or val, dset: train_dataset or val_dataset\n",
    "    filename = split+\".bin\"\n",
    "    length = np.sum(dset[\"len\"], dtype=np.uint64) #データの長さ\n",
    "    write_data = np.memmap(filename, dtype = np.uint16, mode = \"w+\", shape = (length,)) #Vocabが50257サイズなのでuint16で事足りる\n",
    "    iteration = 1024\n",
    "    index = 0\n",
    "    for iter_index in tqdm(range(iteration)):\n",
    "        add_data = dset.shard(num_shards=iteration, index = iter_index, contiguous=True).with_format('numpy')\n",
    "        #dataset.shardはnum_shardsに指定した数だけデータを分割する\n",
    "        add_data = np.concatenate(add_data['ids'])\n",
    "        add_length = len(add_data)\n",
    "        write_data[index:index+add_length] = add_data\n",
    "        index += add_length\n",
    "    write_data.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作したbinファイルはbinと名付けたフォルダに入れた。この先はそういったフォルダ構成を想定して作業をおこなう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作したbinファイルの読み込みもnumpy.memmap関数を用いて行なう。このときmode=\"r\"としておくことに注意して欲しい、でないと上の作業をやり直す羽目になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap(\"bin/train.bin\", dtype = np.uint16, mode = \"r\")\n",
    "val_data = np.memmap(\"bin/val.bin\", dtype = np.uint16, mode = \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データが読み込めたらデータローダーの制作に移る。しかし今回はDataLoaderを使ってデータを\n",
    "取り出すのではなく、<br>関数を用いることとなる。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size = 32\n",
    "batch_size = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def get_batch(split: str, batch_size = 256,device = \"cpu\")->torch.Tensor:\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - sentence_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+sentence_size]).astype(np.int64)) for i in index])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+sentence_size]).astype(np.int64)) for i in index])\n",
    "    if device == \"cuda\":\n",
    "        return x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo get_batchが詳しく何をしているかの説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さてここで、残念なお知らせだが、このリポジトリ製作者のGPUはメモリが8GB程度しかないため、これ以上に学習に工夫を行なう必要がでてきた。<br>\n",
    "手元のパソコンでも実行するためにはまずモデルの縮小を行なう必要があった。OpenAIのオリジナルのGPT1とことなり、<br>\n",
    "今から学習するモデルには以下の変更点がある。<br>\n",
    "- num_headsが12から6に\n",
    "- encoderの繰り返し回数が6回に\n",
    "- 学習時の文章長は1024から32に\n",
    "\n",
    "\n",
    "また、float32のまま学習するとGPUのメモリが足りないため、torch.cuda.amp.GradScaler, torch.amp.autocastを活用することで、<br>\n",
    "省メモリにし、学習を可能にした。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_size = 768\n",
    "num_heads = 6\n",
    "#KarpathyのminGPTを参考に、パラメーターを設定した。\n",
    "gpt = GPT(50257, embedding_size, embedding_size*4, num_heads, 0.1, batch_first=True, T = sentence_size, N = 6).to(device)\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "with torch.amp.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    x,y = get_batch(\"train\",device=device)\n",
    "    padding_mask, mask = gpt.create_mask(x, 0, device)\n",
    "    loss = gpt(x,y)\n",
    "scaler.scale(loss).backward() \n",
    "scaler.step(optimizer) \n",
    "scaler.update()\n",
    "del x, y\n",
    "del padding_mask, mask\n",
    "del loss\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモ\n",
    "https://blogs.nvidia.co.jp/2020/05/26/tensorfloat-32-precision-format/\n",
    "https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587\n",
    "https://qiita.com/Sosuke115/items/40265e6aaf2e414e2fea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
