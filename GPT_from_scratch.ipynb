{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このファイルではGPTモデルの実装を目標とする。このファイルの立ち位置は同リポジトリに含まれている<br>\n",
    "・pytorch_command.ipynb<br>\n",
    "・attention_from_scratch.ipynb<br>\n",
    "の次に読むことを想定されている。pytorchの下位APIは上２つの.ipynbで散々練習したので、上位APIを解禁して最新モデルを組むことを目指す<br>\n",
    "<br>\n",
    "お詫び: このファイルでは最終的に訓練フェースにおいてGPTの制作は叶わなかったので制作をしたい人は途中まで読んだのちに<br>\n",
    "何で製作者が失敗したのかをまとめたlearning_notes.ipynbとImproved_GPT.ipynbを読んでほしい。<br>\n",
    "Improved_GPT.ipynbではこのファイルで制作したデータセットを用いるので、データのダウンロードまではこのファイルで行えば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchに明るい人はこのipynbファイルから読んでも良い、pytorchに明るくない人は上２つのipynbファイルを公式ドキュメントとにらめっこして読むことをおすすめする。<br>\n",
    "内容がわかりにくいと感じた人はGithub上もしくはXで作者に質問/書き換え要請を投げることができる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 本題\n",
    "実装においては[GPT from scratch](https://jaketae.github.io/study/gpt/)やKarpathyの[minGPT](https://github.com/karpathy/minGPT), [nanoGPT](https://github.com/karpathy/nanoGPT)を参考にする。<br>\n",
    "実装に関してはこのipynbを見なくとも紹介した2つのサイトを見れば良い。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装に入る前に簡単なGPTの実装方針についての概要について解説する。今回実装するのは初代GPTである。<br>\n",
    "初代GPTは以下のような構造をしている<br>\n",
    "- GPTはTransformerのDecoderのみを用いたモデルである。\n",
    "- Embedding_dimは768, MultiheadAttentionのHead数は12, TransformerDecoderブロック数は12である。\n",
    "- FFN層の活性化関数はGELUである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練時の注意事項も記載しておく\n",
    "- OptimizerはAdam, 学習率の最大値は2.5e-4, 2000iteratonで最大値に達し、<br>その後はCOSINEスケジューラーによってスケジューリングを行う。attention_from_scratch.ipynbで紹介したwarm_upの改善版のようなものである。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA環境が壊れていないことを祈りながら確認-> True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "print(\"CUDA環境が壊れていないことを祈りながら確認->\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは学習につかうTransformerDecoderLayerを定義する。デフォルトにtorch.nn.TransformerDecoderLayerがあるが、<br>\n",
    "GPTにつかうDecoderLayerは\"Attention is all you need\"で紹介されているDecoder Layerとは少し異なっている。<br>\n",
    "そのため、カスタムレイヤーを定義する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考のために比較画像を用意した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT\n",
    "<img src = \"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.41.44_PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "<img src = \"https://user-images.githubusercontent.com/57289763/160270884-e1901241-a1e6-4890-a5e8-165e87f0c4da.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではGPTのレイヤーをGPTDecoderLayerとして定義しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0., layer_eps=1e-05, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.maskedmultiheadattention = nn.MultiheadAttention(embedding_dim, num_heads,batch_first=batch_first)\n",
    "        self.dropout_selfattn = nn.Dropout(p = drop_out_rate)\n",
    "        self.layernorm_selfattn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, embedding_dim))#GELUに変更\n",
    "        self.layernorm_ffn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "        self.dropout_ffn = nn.Dropout(p = drop_out_rate)\n",
    "\n",
    "    def forward(self, x, pad_mask_self = None, mask_self=None):\n",
    "        dx, _ = self.maskedmultiheadattention(x,x,x,key_padding_mask = pad_mask_self, attn_mask = mask_self)\n",
    "\n",
    "        dx = self.dropout_selfattn(dx)\n",
    "\n",
    "        x = self.layernorm_selfattn(x+dx)\n",
    "\n",
    "        dx = self.dropout_ffn(self.ffn(x))\n",
    "\n",
    "        x = self.layernorm_ffn(x + dx)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作したGPTDecoderLayerにEmbeddingとPositional EncodingをつければGPTモデルの定義が終わる。<br>\n",
    "マスクの制作もGPTクラス内部に含める形で実装を行なう。<br>\n",
    "だがここで注意しなければならなければならないことがある。<br>\n",
    "TransformerモデルではPositional Encodingはsinとcosを用いて実装したが、<br>\n",
    "GPTではこのPositional Encodingも学習可能パラメーターとして実装を行なう。<br>\n",
    "GPTでのこれにあたる層はnn.Embedding(max_sequence_len, embedding_dim)として埋め込み層を定義する。この実装方法はKarpathyの[minGPT](https://github.com/karpathy/minGPT), [nanoGPT](https://github.com/karpathy/nanoGPT)を参考にしたものである。<br>\n",
    "他にもnn.Parameter()で実装するものもあるようである。[GPT from scratch](https://jaketae.github.io/study/gpt/)<br>\n",
    "OpenAIのWhisperなどのモデルではnn.Parameterで実装されていたので、もしかしたらそっちの方が良いかもしれない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward, generate, generate_sentenceについて補足を行う。<br>\n",
    "forwardは入力を$(x_{0},x_{1},...,x_{N})$,出力を$(x_{1},x_{2},...,x_{N+1})$を予測するように作られている。そのため、入力データには未来のデータを使わないようにマスクを行う必要がある。<br>\n",
    "ラベル用データ$y$が入力に入らないときは予測結果の最後、つまり$x_{N+1}$のトークン予測を出力として返すようにしている。<br>\n",
    "二つ目に、generateメソッドでは今までのtokenを元に学習モデルを用いて次の文字を出力している。generate_sentenceメソッドではgenerateメソッドを繰り返してトークンを取り出している。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0.,\\\n",
    "                  layer_eps=1e-05, batch_first = False, T = 10000, N = 1):\n",
    "        super().__init__()\n",
    "        #Tはmax_lenを表している\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,)\n",
    "        self.positional_embedding = nn.Embedding(T, embedding_dim)\n",
    "        self.decoder = nn.ModuleList([GPTDecoderLayer(embedding_dim, ffn_dim, num_heads, drop_out_rate,\\\n",
    "                                                               layer_eps, batch_first) for _ in range(N)])\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size, bias = False)\n",
    "        self.vocab_size = vocab_size\n",
    "    def forward(self, x, y = None,pad_mask_self = None, mask_self=None):\n",
    "        \"\"\"\n",
    "        yはxを1つだけずらしたデータである\n",
    "        x = data[a:b]なら、y = data[a+1:b+1]となる。\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        pos = torch.arange(0,x.size(1),dtype=torch.long).unsqueeze(0).to(x.device)\n",
    "        pos = self.positional_embedding(pos)\n",
    "        x = x + pos\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, pad_mask_self = pad_mask_self, mask_self = mask_self)\n",
    "        x = self.linear(x)\n",
    "        if y != None:\n",
    "            loss = F.cross_entropy(x.view(-1, x.size(-1)), y.view(-1), ignore_index=-1) \n",
    "            #ignore_index=-1はyをonehotベクトル化しないでcross_entropyを使うために使用\n",
    "            pred = x.argmax(dim = -1).detach().cpu()\n",
    "            return torch.clamp(loss, min = 1e-32, max = 10e9), pred\n",
    "        loss = None\n",
    "        pred = x[:,[-1],:].argmax(dim = -1)\n",
    "        return loss, pred\n",
    "    def create_mask(self, x: torch.tensor, x_pad: int, device: str):\n",
    "        \"\"\"\n",
    "        (batch_size, sequence_length, embedding_dim)の入力を想定\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Trueが無視される値であることに注意すること\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        #srcのマスク制作\n",
    "        padding_mask = (x == x_pad)\n",
    "        mask = torch.triu(torch.ones(size = (seq_len, seq_len))==1).transpose(0,1) #下三角行列を作る\n",
    "        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask==1.,float(0.0)).to(device)\n",
    "        return padding_mask, mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,bos: str, sentence_size, tokenizer, device):\n",
    "        self.eval()\n",
    "        bos_tokenized = tokenizer.encode_ordinary(bos)\n",
    "        bos_tokenized = bos_tokenized[-sentence_size:]\n",
    "        bos_tokenized = torch.LongTensor([bos_tokenized])\n",
    "        _, add_sentence = self(bos_tokenized.to(device))\n",
    "        self.train()\n",
    "        return add_sentence\n",
    "    @torch.no_grad()\n",
    "    def generate_sentence(self, bos: str, sentence_size, generate_tokens, tokenizer, device):\n",
    "        return_sentence = bos\n",
    "        for i in range(generate_tokens):\n",
    "            add_sentence = self.generate(return_sentence, sentence_size, tokenizer,device)\n",
    "            return_sentence += tokenizer.decode_batch(add_sentence.tolist())[0]\n",
    "        return return_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上手く動くか試しに検証してみよう。\n",
    "入力は(batch_size, tokens) = (1, 6)のLongTensorで、create_maskでmaskを製作し、上手く動作することを確かめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.LongTensor([[2,10,20,100,512,3],[2,10,20,100,512,3],[2,10,20,100,512,3]]).to(device)\n",
    "x = x.reshape(3,6)\n",
    "embedding_size = 768\n",
    "num_heads = 12\n",
    "#KarpathyのminGPTを参考に、パラメーターを設定した。\n",
    "gpt = GPT(50257, embedding_size, embedding_size*4, num_heads, 0.1, batch_first=True, T = 1024, N = 12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      " tensor(10.6196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Pred: \n",
      " tensor([[18577, 26818,  1185, 41473,  5617,  1185],\n",
      "        [ 1185, 26818,  1185, 17752, 42762,  3401]])\n"
     ]
    }
   ],
   "source": [
    "padding_mask, mask = gpt.create_mask(x[0:2], 0, device)\n",
    "loss, pred = gpt(x[0:2],x[1:3],padding_mask,mask)\n",
    "print(\"Loss: \\n\",loss)\n",
    "print(\"Pred: \\n\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの構造は以下の通りである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (embedding): Embedding(50257, 768)\n",
      "  (positional_embedding): Embedding(1024, 768)\n",
      "  (decoder): ModuleList(\n",
      "    (0-11): 12 x GPTDecoderLayer(\n",
      "      (maskedmultiheadattention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout_selfattn): Dropout(p=0.1, inplace=False)\n",
      "      (layernorm_selfattn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (layernorm_ffn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout_ffn): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "気になるパラメーター数は以下のとおりである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters is  163035648\n"
     ]
    }
   ],
   "source": [
    "count_params = 0\n",
    "for params in gpt.parameters():\n",
    "    count_params += params.contiguous().view(-1).size(0)\n",
    "print(\"The number of parameters is \", count_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT1のパラメーター数は1億程度であると言われているが、実際に組んだモデルでも1億程度になっていることが確かめられた。<br>\n",
    "途方もない数字のように思えるが、全てのパラメーターがfloat32(4バイト)であることを考えて<br>簡単な計算を行なうと,\n",
    "家にある普通のGPUでもモデルのパラメーターを乗せることができるとわかる。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#念の為メモリを開放しておく\n",
    "import gc\n",
    "del gpt\n",
    "del x\n",
    "del padding_mask, mask\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でモデルの定義は終わりである。ここからはデータセットの定義に移る。<br>\n",
    "GPTモデルでは分かち書きに工夫がなされている。これはByte-Pair Encodingと言われている。<br>\n",
    "実装はやや複雑なので別ファイルで解説を行なうかもしれない。ここではtiktokenライブラリのGPT2Tokenizerを利用しようと思う。<br>\n",
    "tiktokenはOpenAIから提供されているライブラリであり、huggingfaceから提供されているGPT2Tokenizerよりも<br>\n",
    "早いことが知られている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 257, 6291, 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#本当は実装したかったけどヤムナシ\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer.encode_ordinary(\"This is a sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの定義を行なう。データはopenwebtextを用いることにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "num_proc_load_dataset = 8 #並列処理をどの程度行なうか\n",
    "dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしたらpickleで保存しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.bin\",\"wb\") as p:\n",
    "    pickle.dump(dataset,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.bin\", \"rb\") as p:\n",
    "    dataset = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ以降はデータの処理のためにHugging FaceのDatasetDictに提供されている<br>\n",
    "前処理メソッドを使って前処理を行っていきます。各メソッドの解説は[HuggingFace datasets process](https://huggingface.co/docs/datasets/process#map)にあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset分割\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test') # rename the test split to val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8009762\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットをtrainとvalに分割したため、tokenizeを行います。<br>\n",
    "mapメソッドは第一引数に辞書型を返す処理関数を与えます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing the splits (num_proc=8): 100%|██████████| 8009762/8009762 [21:33<00:00, 6194.09 examples/s]\n",
      "tokenizing the splits (num_proc=8): 100%|██████████| 4007/4007 [00:01<00:00, 2138.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def process(example):\n",
    "        ids = tokenizer.encode_ordinary(example['text']) \n",
    "        ids.append(tokenizer.eot_token) #文末に<endoftext>tokenを追加\n",
    "        #\n",
    "        out = {'ids': ids, 'len': len(ids)}\n",
    "        return out\n",
    "\n",
    "\n",
    "tokenized = split_dataset.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=num_proc_load_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ids', 'len'],\n",
       "        num_rows: 8009762\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['ids', 'len'],\n",
       "        num_rows: 4007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizeを行ったら、同じようにpickleで保存しておく。ここまでで製作したdataset.binとtokenized_dataset.binはgithub内部にもおいておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"tokenized_dataset.bin\",\"wb\") as p:\n",
    "    pickle.dump(tokenized,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"tokenized_dataset.bin\", \"rb\") as p:\n",
    "    tokenized = pickle.load(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizeが終了したデータは一つのbinファイルにまとめる。このとき、train dataだけで800万データがあるため、<br>\n",
    "メモリ上に全データが乗らない場合がある。こんなときはnumpy.memmapを使ってファイルに書き出しを行なう。<br>\n",
    "numpy.memmap(filename, dtype, mode, shape)を指定すると、指定したファイルの置き場所にファイルが生成される。<br>\n",
    "製作したnumpy.memmapファイルはnumpy.ndarrayと同じように値が書き込めるため、800万データを少しずつ書き込めばよい。<br>\n",
    "更に詳しい説明は公式ドキュメントの[numpy.memmap](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html)を参考にして欲しい。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [02:35<00:00,  6.59it/s]\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 1293.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for split, dset in tokenized.items():\n",
    "    #split: train or val, dset: train_dataset or val_dataset\n",
    "    filename = split+\".bin\"\n",
    "    length = np.sum(dset[\"len\"], dtype=np.uint64) #データの長さ\n",
    "    write_data = np.memmap(filename, dtype = np.uint16, mode = \"w+\", shape = (length,)) #Vocabが50257サイズなのでuint16で事足りる\n",
    "    iteration = 1024\n",
    "    index = 0\n",
    "    for iter_index in tqdm(range(iteration)):\n",
    "        add_data = dset.shard(num_shards=iteration, index = iter_index, contiguous=True).with_format('numpy')\n",
    "        #dataset.shardはnum_shardsに指定した数だけデータを分割する\n",
    "        add_data = np.concatenate(add_data['ids'])\n",
    "        add_length = len(add_data)\n",
    "        write_data[index:index+add_length] = add_data\n",
    "        index += add_length\n",
    "    write_data.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作したbinファイルはbinと名付けたフォルダに入れた。この先はそういったフォルダ構成を想定して作業をおこなう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作したbinファイルの読み込みもnumpy.memmap関数を用いて行なう。このときmode=\"r\"としておくことに注意して欲しい、でないと上の作業をやり直す羽目になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap(\"bin/train.bin\", dtype = np.uint16, mode = \"r\")\n",
    "val_data = np.memmap(\"bin/val.bin\", dtype = np.uint16, mode = \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データが読み込めたらデータローダーの制作に移る。しかし今回はDataLoaderを使ってデータを\n",
    "取り出すのではなく、<br>関数を用いることとなる。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size = 1024\n",
    "batch_size = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def get_batch(split: str, batch_size = 256,device = \"cpu\")->torch.Tensor:\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - sentence_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+sentence_size]).astype(np.int64)) for i in index])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+sentence_size]).astype(np.int64)) for i in index])\n",
    "    if device == \"cuda\":\n",
    "        return x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_batch関数は(dataのサイズ-sentence_size)までのindexをbatch_sizeだけ生成し、batch_size分だけ、<br>\n",
    "index:index+sentence_sizeのデータを入力データ、index+1:index+sentence_size+1のデータをラベルとしている。<br>\n",
    "モデルのforwardは入力を$(x_{0},x_{1},...,x_{N})$,出力を$(x_{1},x_{2},...,x_{N+1})$を予測するように作られていることに対応している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さてここで、残念なお知らせだが、このリポジトリ製作者のGPUはメモリが8GB程度しかないため、これ以上に学習に工夫を行なう必要がでてきた。<br>\n",
    "手元のパソコンでも実行するためにはまずモデルの縮小を行なう必要があった。OpenAIのオリジナルのGPT1とことなり、<br>\n",
    "今から学習するモデルには以下の変更点がある。<br>\n",
    "- num_headsが12から6に\n",
    "- encoderの繰り返し回数が6回に\n",
    "\n",
    "\n",
    "また、float32のまま学習するとGPUのメモリが足りないため、torch.cuda.amp.GradScaler, torch.amp.autocastを活用することで、<br>\n",
    "省メモリにし、学習を可能にした。詳しくはlearning_notes.ipynbに記した。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_size = 768\n",
    "num_heads = 6\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "#KarpathyのminGPTを参考に、パラメーターを設定した。\n",
    "depth = 6\n",
    "gpt = GPT(50257, embedding_size, embedding_size*4, num_heads, 0, batch_first=True, T = sentence_size, N = depth).to(device) \n",
    "#事前学習のときはDropout無し、ファインチューニングのときはありが好ましい\n",
    "warmup_iters = 2000\n",
    "\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の際に用いるCOS schedulerの定義を行なう。<br>\n",
    "cos schedulerの定義式はwarm_upとmax_iterを用いて以下のとおりに表される。<br>\n",
    "$$\n",
    "\\text{learning rate} = \\text{max\\_learning\\_rate} \\times \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\dfrac{\\text{epoch}}{\\text{warm up}} & \\text{epoch} \\lt \\text{warm up} \\\\\n",
    "\\cos{(\\dfrac{\\text{epoch}}{\\text{max\\_iter}}\\pi)} & \\text{epoch} \\ge \\text{warm up}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、このschedulerはepochの最後の方になるほどパラメーターの調整度合いが非常に低くなり、学習の意味がなくなってしまう。<br>\n",
    "このため、学習率の最小値を決めておき、今回は以下の更新式で実装を行なう。\n",
    "$$\n",
    "\\text{learning rate} = \\text{max\\_learning\\_rate} \\times \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\dfrac{\\text{epoch}}{\\text{warm up}} & \\text{epoch} \\lt \\text{warm up} \\\\\n",
    "\\min(\\cos{(\\dfrac{\\text{epoch}}{\\text{max\\_iter}}\\pi)}, \\text{min\\_lr}) & \\text{epoch} \\ge \\text{warm up}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max = 2.5e-4\n",
    "min_lr = 2.5e-5\n",
    "max_iters = 10000\n",
    "def get_lr(cur_iter):\n",
    "    #cur_iter現在のiteration\n",
    "    if cur_iter < warmup_iters:\n",
    "        return lr_max * cur_iter / warmup_iters\n",
    "    return max(lr_max * (np.cos(cur_iter / max_iters * np.pi) + 1), min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "iteration = np.arange(max_iters+1)\n",
    "lr = np.array([get_lr(i) for i in iteration])\n",
    "plt.plot(iteration, lr)\n",
    "plt.plot(iteration, [min_lr]*len(iteration))\n",
    "plt.legend([\"learning rate\"],[\"min_lr\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はbatchの繰り返し回数を96回、最大epochを10000、学習状況を逐一保存することで途中からの開始が可能になるようにして学習をおこなった。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "batch_iteration = 96\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "best_loss = 1e9\n",
    "begin = 0\n",
    "for cur_iter in tqdm(range(begin,max_iters)):\n",
    "    optimizer.lr = get_lr(cur_iter+1)\n",
    "    for batch_iter in range(batch_iteration):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
    "            x,y = get_batch(\"train\",batch_size=batch_size,device=device)\n",
    "            padding_mask, mask = gpt.create_mask(x, 0, device)\n",
    "            loss, pred = gpt(x,y,padding_mask,mask)\n",
    "        scaler.scale(loss).backward() \n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "        if best_loss > loss.detach().item():\n",
    "            best_loss = loss.detach().item()\n",
    "            torch.save(gpt.state_dict(), \"gpt.pth\")\n",
    "            print(\"params updated.\")\n",
    "            with open(\"learning_detail.txt\",\"w\") as f:\n",
    "                f.write(\"学習状況\\n\")\n",
    "                f.write(f\"iter: {cur_iter}\\n\")\n",
    "                f.write(f\"hyper params: \\n\")\n",
    "                f.write(f\"vocab_size: 50257, embedding size: {embedding_size}, ffn: {embedding_size*4}, num_heads: {num_heads}, Depth: {depth}, sentnce_size: {sentence_size}\\n\")\n",
    "                f.write(f\"lr: {optimizer.lr},best_loss: {best_loss}\\n\")\n",
    "                f.close()\n",
    "        del x, y\n",
    "        del padding_mask, mask\n",
    "        del loss\n",
    "        del pred\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    with open(\"learning_detail_latest.txt\",\"w\") as f:\n",
    "        f.write(\"学習状況\\n\")\n",
    "        f.write(f\"iter: {cur_iter}\\n\")\n",
    "        f.write(f\"hyper params: \\n\")\n",
    "        f.write(f\"vocab_size: 50257, embedding size: {embedding_size}, ffn: {embedding_size*4}, num_heads: {num_heads}, Depth: {depth}, sentnce_size: {sentence_size}\\n\")\n",
    "        f.write(f\"lr: {optimizer.lr},best_loss: {best_loss}\\n\")\n",
    "        f.close()\n",
    "    torch.save(gpt.state_dict(), \"gpt_latest.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "長いので一旦283/10000で学習を打ち切った。この状態での文章抽出を見てみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.generate_sentence(\"This is\", sentence_size, 100, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは明らかに失敗している。この状態ではまだ不十分なGPTになってしまうと考えられる。<br>\n",
    "学習経過を観察した結果、100エポックを超えたあたりから損失があまり小さくならない勾配消失が起きていることが考えられた。<br>\n",
    "これを改善したファイルがImproved_GPT.ipynbである。改善されたGPTを制作するために次はそちらを読むことをおすすめする。<br>\n",
    "また、今回学習が失敗したと考えられる要因をlearning_notes.ipynbにまとめておいたため、そちらも読むと良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考にしたサイトなど<br>\n",
    "[https://blogs.nvidia.co.jp/2020/05/26/tensorfloat-32-precision-format/](https://blogs.nvidia.co.jp/2020/05/26/tensorfloat-32-precision-format/)<br>\n",
    "[https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587](https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587)<br>\n",
    "[https://qiita.com/Sosuke115/items/40265e6aaf2e414e2fea](https://qiita.com/Sosuke115/items/40265e6aaf2e414e2fea)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
