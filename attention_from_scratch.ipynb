{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention from scratch\n",
    "ここではpytorch_command.ipynbで学んだことを活かしてAttentionの実装を行います。<br>\n",
    "### 目次<br>\n",
    "- Attentionが生まれた経緯\n",
    "- Self-Attention, Multi-head Attention, Transformerの理論、実装\n",
    "- Transformer機構を用いた日英翻訳\n",
    "- Appendix Hugging Faceの扱い方<br>\n",
    "Attention機構を用いた翻訳を作る際に使うデータセットはHugging Faceのものを扱います。https://huggingface.co/datasets/snow_simplified_japanese_corpus<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attentionが生まれた経緯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attntion機構が初めて世の中に発表されたのは\"\"Neural Machine Translation by Jointly Learning to Align and Translate\"\"という論文であるとされている。<br>\n",
    "この論文はRNNに注意という概念を加えて機械翻訳の性能を向上させたものである。<br>\n",
    "この論文内ではAttentionという名前は使われていなく、Alignという表記がされていることに注意してほしい。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionが生まれた以前の機械翻訳は深層学習一強というわけでもなく、Encoder-Decoderを用いたRNNベースの深層学習だったり、<br>\n",
    "Mosesと呼ばれる統計的機会翻訳が強かったりとどれが一番強い手法なのかを競い合っていた。<br>\n",
    "「Neural Machine Translation by Jointly Learning to Align and Translate」が公開されたのは2014年であり、<br>\n",
    "Google翻訳も2016年から処理を深層学習ベースに変えたとwikipediaにもある。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "統計的機械翻訳と深層学習ベース機械翻訳の比較は他の人に任せるとして、<br>\n",
    "ここでは既存のSeq2Seqの機械翻訳に比べてAttentionのどこが優位であったのかを簡単に述べる。<br>\n",
    "Seq2Seqモデルの問題点<br>\n",
    "- 入力する文章の長さに関わらず固定長ベクトルに圧縮するため、必要な情報が捨象されてしまう。<br>\n",
    "- 単語、文章同士の照応関係を利用できない。<br>\n",
    "- 学習データにある文字列長以上の文字列を入力すると上手く機能しない。<br>\n",
    "<br>\n",
    "これらの問題点を理解するのは実際にSeq2Seqモデルを組んで実行してみるといいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの問題点を緩和したものがRNNにAttention機構を追加したものであり、RNNを使わないことでRNN由来の問題点を解消し、<br>\n",
    "更に精度を向上させたものがMulti-head Attentionを用いたTransformerとなります。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention, Multi-head Attention, Transformerの理論、実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-Attentionメカニズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self−Attentionメカニズムは3つのステージで構成されている。<br>\n",
    "1. 入力シーケンス$x^{(1)},x^{(2)},...,x^{(t)}$のうち、現在の要素$x^{(i)}$と、他の全要素$x^{(j)}$との間の類似度に基づいて重要度$ω_{ij}$を計算する<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを式で表現すると以下の通りとなる<br>\n",
    "$$\n",
    "ω_{ij} = {x^{(i)}}^{T}x^{(j)}\n",
    "$$\n",
    "  計画行列は\n",
    "$$\n",
    "X = \\begin{pmatrix} \n",
    "  {x^{(1)}}^{T} \\\\\n",
    "  {x^{(2)}}^{T} \\\\\n",
    "  \\vdots\\\\\n",
    "  \\vdots\\\\\n",
    "  {x^{(t)}}^{T} \n",
    "\\end{pmatrix}, X^{T} = \\begin{pmatrix} x^{(1)},  x^{(2)},\\dots,x^{(t)} \\end{pmatrix}\n",
    "$$\n",
    "と表されることを考えると、$Ω = {(ω_{ij})_{i,j}}$という$ω_{ij}$をまとめた行列を求めるときには\n",
    "$$\n",
    "Ω = XX^{T}\n",
    "$$\n",
    "を計算すれば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. 1.で求めた$ω_{ij}$を正規化して$α_{ij}$を求める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式で表現すると<br>\n",
    "$$\n",
    "α_{ij} = \\dfrac{\\exp{(\\omega_{ij})}}{\\displaystyle \\sum_{k=1}^{t}\\exp{(\\omega_{ik})}}\n",
    "$$\n",
    "である。実装の際はpytorchで効率よく計算を行う\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 重みをシーケンス内の対応する要素と組み合わせてAttentionスコアを計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式で表現すると、\n",
    "$$\n",
    "z^{(i)} = \\displaystyle \\sum_{j = 1}^{t} \\alpha_{ij}x^{(j)}\n",
    "$$\n",
    "である。実装の際はpytorchで効率よく計算を行う。この$z^{(i)}$がSelf-Attentionの出力となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算についてもう少し詳しく見ていくと、<br>\n",
    "$$\n",
    "z_{ji} = \\alpha_{i1}x_{1j} + \\alpha_{i2}x_{2j} + \\dots + \\alpha_{it}x_{tj}\n",
    "$$\n",
    "である。これは、\n",
    "$$\n",
    "Z = \\begin{pmatrix} \n",
    "  {z^{(1)}}^{T} \\\\\n",
    "  {z^{(2)}}^{T} \\\\\n",
    "  \\vdots\\\\\n",
    "  \\vdots\\\\\n",
    "  {z^{(t)}}^{T} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "としたときに、\n",
    "$$\n",
    "Z = AX, \\text{where} \\, A = (\\alpha_{ij})_{i,j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "を計算することにあたる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの処理を詳しく見ていこう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 重要度$\\omega_{ij}$の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既に辞書などを用いて単語は整数表現にマッピングされているものとする。<br>\n",
    "このとき、[3,5,1,6,8,3,6,4]と表されるような文章を計算の対象とする。<br>\n",
    "まずはこの文のベクトル表現をnn.Embeddingで取得する。dictionary_sizeは10, embedding_sizeは16とする。<br>\n",
    "もしここまでで分散表現や埋め込みという表現に馴染みがなかった場合は自然言語処理の前処理について調べてみると良い<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリの定義、シード値の固定\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成された埋め込みベクトル:\n",
      " tensor([[-9.1382e-01, -6.5814e-01,  7.8024e-02,  5.2581e-01, -4.8799e-01,\n",
      "          1.1914e+00, -8.1401e-01, -7.3599e-01, -1.4032e+00,  3.6004e-02,\n",
      "         -6.3477e-02,  6.7561e-01, -9.7807e-02,  1.8446e+00, -1.1845e+00,\n",
      "          1.3835e+00],\n",
      "        [ 1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01,  5.3619e-01,\n",
      "          5.2462e-01,  1.1412e+00,  5.1644e-02,  7.4395e-01, -4.8158e-01,\n",
      "         -1.0495e+00,  6.0390e-01, -1.7223e+00, -8.2777e-01,  1.3347e+00,\n",
      "          4.8354e-01],\n",
      "        [ 1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01, -7.5813e-01,\n",
      "          1.0783e+00,  8.0080e-01,  1.6806e+00,  1.2791e+00,  1.2964e+00,\n",
      "          6.1047e-01,  1.3347e+00, -2.3162e-01,  4.1759e-02, -2.5158e-01,\n",
      "          8.5986e-01],\n",
      "        [-2.5095e+00,  4.8800e-01,  7.8459e-01,  2.8647e-02,  6.4076e-01,\n",
      "          5.8325e-01,  1.0669e+00, -4.5015e-01, -1.8527e-01,  7.5276e-01,\n",
      "          4.0476e-01,  1.7847e-01,  2.6491e-01,  1.2732e+00, -1.3109e-03,\n",
      "         -3.0360e-01],\n",
      "        [ 1.9312e+00,  1.0119e+00, -1.4364e+00, -1.1299e+00, -1.3603e-01,\n",
      "          1.6354e+00,  6.5474e-01,  5.7600e-01,  1.1415e+00,  1.8565e-02,\n",
      "         -1.8058e+00,  9.2543e-01, -3.7534e-01,  1.0331e+00, -6.8665e-01,\n",
      "          6.3681e-01],\n",
      "        [-9.1382e-01, -6.5814e-01,  7.8024e-02,  5.2581e-01, -4.8799e-01,\n",
      "          1.1914e+00, -8.1401e-01, -7.3599e-01, -1.4032e+00,  3.6004e-02,\n",
      "         -6.3477e-02,  6.7561e-01, -9.7807e-02,  1.8446e+00, -1.1845e+00,\n",
      "          1.3835e+00],\n",
      "        [-2.5095e+00,  4.8800e-01,  7.8459e-01,  2.8647e-02,  6.4076e-01,\n",
      "          5.8325e-01,  1.0669e+00, -4.5015e-01, -1.8527e-01,  7.5276e-01,\n",
      "          4.0476e-01,  1.7847e-01,  2.6491e-01,  1.2732e+00, -1.3109e-03,\n",
      "         -3.0360e-01],\n",
      "        [ 1.4451e+00,  8.5641e-01,  2.2181e+00,  5.2317e-01,  3.4665e-01,\n",
      "         -1.9733e-01, -1.0546e+00,  1.2780e+00, -1.7219e-01,  5.2379e-01,\n",
      "          5.6622e-02,  4.2630e-01,  5.7501e-01, -6.4172e-01, -2.2064e+00,\n",
      "         -7.5080e-01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "x = torch.Tensor([3,5,1,6,8,3,6,4]).int()\n",
    "embed = nn.Embedding(10,16)\n",
    "embed_x = embed(x)\n",
    "print(\"生成された埋め込みベクトル:\\n\", embed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omega: \n",
      " tensor([[13.5729, -3.6601, -0.7355,  4.1794,  0.6965, 13.5729,  4.1794, -1.0428],\n",
      "        [-3.6601, 12.0409,  2.5783, -1.8934,  6.3044, -3.6601, -1.8934, -8.5589],\n",
      "        [-0.7355,  2.5783, 14.6957, -3.3807,  9.0543, -0.7355, -3.3807,  3.0030],\n",
      "        [ 4.1794, -1.8934, -3.3807, 11.8240, -3.9458,  4.1794, 11.8240, -2.9556],\n",
      "        [ 0.6965,  6.3044,  9.0543, -3.9458, 19.0526,  0.6965, -3.9458, -0.1805],\n",
      "        [13.5729, -3.6601, -0.7355,  4.1794,  0.6965, 13.5729,  4.1794, -1.0428],\n",
      "        [ 4.1794, -1.8934, -3.3807, 11.8240, -3.9458,  4.1794, 11.8240, -2.9556],\n",
      "        [-1.0428, -8.5589,  3.0030, -2.9556, -0.1805, -1.0428, -2.9556, 17.5832]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega = embed_x @ embed_x.transpose(0,1)\n",
    "print(\"omega: \\n\", omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 1.で求めた$ω_{ij}$を正規化して$α_{ij}$を求める。<br>\n",
    "計算の際は愚直に計算をするか、torch.nn.functionalのsoftmax関数を利用する。愚直に計算をする場合はkeepdim引数を忘れないこと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "愚直に実装を行った場合: \n",
      " tensor([[4.9996e-01, 1.6396e-08, 3.0542e-07, 4.1630e-05, 1.2788e-06, 4.9996e-01,\n",
      "         4.1630e-05, 2.2461e-07],\n",
      "        [1.5126e-07, 9.9670e-01, 7.7451e-05, 8.8509e-07, 3.2154e-03, 1.5126e-07,\n",
      "         8.8509e-07, 1.1277e-09],\n",
      "        [1.9805e-07, 5.4442e-06, 9.9645e-01, 1.4059e-08, 3.5352e-03, 1.9805e-07,\n",
      "         1.4059e-08, 8.3246e-06],\n",
      "        [2.3919e-04, 5.5126e-07, 1.2457e-07, 4.9976e-01, 7.0797e-08, 2.3919e-04,\n",
      "         4.9976e-01, 1.9058e-07],\n",
      "        [1.0667e-08, 2.9075e-06, 4.5476e-05, 1.0278e-10, 9.9995e-01, 1.0667e-08,\n",
      "         1.0278e-10, 4.4379e-09],\n",
      "        [4.9996e-01, 1.6396e-08, 3.0542e-07, 4.1630e-05, 1.2788e-06, 4.9996e-01,\n",
      "         4.1630e-05, 2.2461e-07],\n",
      "        [2.3919e-04, 5.5126e-07, 1.2457e-07, 4.9976e-01, 7.0798e-08, 2.3919e-04,\n",
      "         4.9976e-01, 1.9058e-07],\n",
      "        [8.1439e-09, 4.4321e-12, 4.6546e-07, 1.2026e-09, 1.9289e-08, 8.1439e-09,\n",
      "         1.2026e-09, 1.0000e+00]], grad_fn=<DivBackward0>)\n",
      "torch.nn.functional.softmaxを使った場合: \n",
      " tensor([[4.9996e-01, 1.6396e-08, 3.0542e-07, 4.1630e-05, 1.2788e-06, 4.9996e-01,\n",
      "         4.1630e-05, 2.2461e-07],\n",
      "        [1.5126e-07, 9.9671e-01, 7.7451e-05, 8.8509e-07, 3.2154e-03, 1.5126e-07,\n",
      "         8.8509e-07, 1.1277e-09],\n",
      "        [1.9805e-07, 5.4442e-06, 9.9645e-01, 1.4059e-08, 3.5352e-03, 1.9805e-07,\n",
      "         1.4059e-08, 8.3246e-06],\n",
      "        [2.3919e-04, 5.5126e-07, 1.2457e-07, 4.9976e-01, 7.0798e-08, 2.3919e-04,\n",
      "         4.9976e-01, 1.9058e-07],\n",
      "        [1.0667e-08, 2.9075e-06, 4.5476e-05, 1.0278e-10, 9.9995e-01, 1.0667e-08,\n",
      "         1.0278e-10, 4.4379e-09],\n",
      "        [4.9996e-01, 1.6396e-08, 3.0542e-07, 4.1630e-05, 1.2788e-06, 4.9996e-01,\n",
      "         4.1630e-05, 2.2461e-07],\n",
      "        [2.3919e-04, 5.5126e-07, 1.2457e-07, 4.9976e-01, 7.0798e-08, 2.3919e-04,\n",
      "         4.9976e-01, 1.9058e-07],\n",
      "        [8.1439e-09, 4.4321e-12, 4.6546e-07, 1.2026e-09, 1.9289e-08, 8.1439e-09,\n",
      "         1.2026e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "print(\"愚直に実装を行った場合: \\n\", torch.exp(omega) / torch.exp(omega).sum(dim=1,keepdim=True))\n",
    "print(\"torch.nn.functional.softmaxを使った場合: \\n\",F.softmax(omega, dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 重みをシーケンス内の対応する要素と組み合わせてAttentionスコアを計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Score: \n",
      " tensor([[-9.1395e-01, -6.5804e-01,  7.8081e-02,  5.2577e-01, -4.8790e-01,\n",
      "          1.1913e+00, -8.1385e-01, -7.3597e-01, -1.4031e+00,  3.6064e-02,\n",
      "         -6.3440e-02,  6.7557e-01, -9.7777e-02,  1.8445e+00, -1.1844e+00,\n",
      "          1.3834e+00],\n",
      "        [ 1.7164e-02, -3.3438e-01, -1.3409e+00, -5.8704e-01,  5.3393e-01,\n",
      "          5.2824e-01,  1.1396e+00,  5.3455e-02,  7.4527e-01, -4.7984e-01,\n",
      "         -1.0518e+00,  6.0499e-01, -1.7178e+00, -8.2171e-01,  1.3281e+00,\n",
      "          4.8406e-01],\n",
      "        [ 1.6433e+00, -1.5545e-01, -5.0070e-01,  4.3404e-01, -7.5592e-01,\n",
      "          1.0803e+00,  8.0027e-01,  1.6767e+00,  1.2786e+00,  1.2919e+00,\n",
      "          6.0191e-01,  1.3333e+00, -2.3213e-01,  4.5254e-02, -2.5312e-01,\n",
      "          8.5905e-01],\n",
      "        [-2.5088e+00,  4.8745e-01,  7.8425e-01,  2.8885e-02,  6.4022e-01,\n",
      "          5.8354e-01,  1.0660e+00, -4.5029e-01, -1.8585e-01,  7.5242e-01,\n",
      "          4.0453e-01,  1.7870e-01,  2.6473e-01,  1.2734e+00, -1.8767e-03,\n",
      "         -3.0280e-01],\n",
      "        [ 1.9311e+00,  1.0118e+00, -1.4364e+00, -1.1298e+00, -1.3606e-01,\n",
      "          1.6354e+00,  6.5475e-01,  5.7605e-01,  1.1415e+00,  1.8621e-02,\n",
      "         -1.8057e+00,  9.2545e-01, -3.7534e-01,  1.0330e+00, -6.8663e-01,\n",
      "          6.3682e-01],\n",
      "        [-9.1395e-01, -6.5804e-01,  7.8081e-02,  5.2577e-01, -4.8790e-01,\n",
      "          1.1913e+00, -8.1385e-01, -7.3597e-01, -1.4031e+00,  3.6064e-02,\n",
      "         -6.3440e-02,  6.7557e-01, -9.7777e-02,  1.8445e+00, -1.1844e+00,\n",
      "          1.3834e+00],\n",
      "        [-2.5088e+00,  4.8745e-01,  7.8425e-01,  2.8885e-02,  6.4022e-01,\n",
      "          5.8354e-01,  1.0660e+00, -4.5029e-01, -1.8585e-01,  7.5242e-01,\n",
      "          4.0453e-01,  1.7870e-01,  2.6473e-01,  1.2734e+00, -1.8767e-03,\n",
      "         -3.0280e-01],\n",
      "        [ 1.4451e+00,  8.5641e-01,  2.2181e+00,  5.2317e-01,  3.4665e-01,\n",
      "         -1.9733e-01, -1.0546e+00,  1.2780e+00, -1.7219e-01,  5.2379e-01,\n",
      "          5.6622e-02,  4.2630e-01,  5.7500e-01, -6.4172e-01, -2.2064e+00,\n",
      "         -7.5080e-01]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weight = F.softmax(omega, dim = 1)\n",
    "z = attention_weight @ embed_x\n",
    "print(\"Attention Score: \\n\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot product Attentionの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今まで解説してきたself attentionでは出力を計算する際に学習可能なパラメーターを使わなかった。<br>\n",
    "そのため。Attention Scoreの計算においてある程度の制限がある。<br>\n",
    "そこで、Self Attentionメカニズムがモデルの最適化により柔軟に対応できるようにするために、モデルの訓練パラメーターとして<br>\n",
    "3つの重み行列をSelf Attentionメカニズムに追加する。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つの重み行列をかけたものをそれぞれquery, key, valueとすると、<br>\n",
    "$$\n",
    "q^{(i)} = U_{q}x^{(i)} \\\\\n",
    "k^{(i)} = U_{k}x^{(i)} \\\\\n",
    "v^{(i)} = U_{v}x^{(i)} \\\\\n",
    "$$\n",
    "という形でそれぞれquery, key, valueは表現される。<br>\n",
    "このときに使われる行列$U_{q}$. $U_{k}$, $U_{v}$は射影行列であり、そのサイズは、$x^{(i)} \\in \\mathbb{R}^d$とすると、それぞれ\n",
    "<br>$d_{k} \\times d$, $d_{k} \\times d$, $d_{v} \\times d$となる。$U_q$と$U_k$のサイズが同じことに注意してほしい。<br>\n",
    "計算された$q, k, v$を用いて、重要度$\\omega_{ij}$は以下のように計算される。<br>\n",
    "$$\n",
    "\\omega_{ij} = {q^{(i)}}^{T}k^{(j)}\n",
    "$$\n",
    "次に正規化して$\\alpha_{ij}$を求める。$\\exp{(\\dfrac{\\omega_{ij}}{\\sqrt{d_k}})}$とスケーリングを行うことで重みベクトルのユークリッド距離がだいたい同じ範囲に収まるようにする<br>\n",
    "$$\n",
    "\\alpha_{ij} = \\text{softmax}(\\dfrac{\\omega_{ij}}{\\sqrt{d_{k}}}) = \\dfrac{\\exp{(\\dfrac{\\omega_{ij}}{\\sqrt{d_k}})}}{\\displaystyle \\sum_{l=1}^{t}\\exp{(\\dfrac{\\omega_{il}}{\\sqrt{d_k}})}}\n",
    "$$\n",
    "最後に出力を計算する。\n",
    "$$\n",
    "z^{(i)} = \\displaystyle \\sum_{j=1}^{t}\\alpha_{ij}v^{(j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上までの処理を行列で表す。<br>\n",
    "$$ Q = \\begin{pmatrix}   {q^{(1)}}^{T} \\\\  {q^{(2)}}^{T} \\\\  \\vdots\\\\  \\vdots\\\\  {q^{(t)}}^{T} \\end{pmatrix}, K = \\begin{pmatrix}   {k^{(1)}}^{T} \\\\  {k^{(2)}}^{T} \\\\  \\vdots\\\\  \\vdots\\\\  {k^{(t)}}^{T} \\end{pmatrix}, V = \\begin{pmatrix}   {v^{(1)}}^{T} \\\\  {v^{(2)}}^{T} \\\\  \\vdots\\\\  \\vdots\\\\  {v^{(t)}}^{T} \\end{pmatrix}, Ω = {(ω_{ij})_{i,j}}, A = {(\\alpha_{ij})_{i,j}}, \n",
    "Z = \\begin{pmatrix} \n",
    "  {z^{(1)}}^{T} \\\\\n",
    "  {z^{(2)}}^{T} \\\\\n",
    "  \\vdots\\\\\n",
    "  \\vdots\\\\\n",
    "  {z^{(t)}}^{T} \n",
    "\\end{pmatrix}$$\n",
    "として定義を行うと、<br>\n",
    "$$\n",
    "Q = (U_qX^T)^{T} = X{U_q}^T, K = X{U_k}^T, V = X{U_v}^T, Ω = QK^T, Z = AV\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "を計算すれば良い。これを図に表したものがよく見る下の図となる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、$Q， K， V$を用いて出力Zを求めることを$Z = \\text{Attention}(Q,K,V)$と表現する文献もある。ここでは以下この記法を用いることもある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png\"><br>\n",
    "### 実装は[Paper with code: Scaled Dot-Product Attention](https://paperswithcode.com/method/scaled)にもある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は入力に対して変換行列をかけることで$Q,K,V$を求めたが、Q,K,Vの求め方によってattentionの名前が異なる。<br>\n",
    "詳しくは次のサイトなどを参考ににしてほしい。[30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen/articles/2023-01-shimizu)<br>\n",
    "以下では区別のために、Scaled dot product self attentionと呼ぶことにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ここでQuery， Key， Valueの説明をしたほうが良い。```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot product self attentionの実装の前にもう少しだけ詳しく実装の仕方を見ていこう。<br>\n",
    "Scaled dot product self attentionは(batch_size, sequence_length, embedding_dim)という入力を想定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(batch_size, sequence_length, embedding_dim) = (2,5,5)を入力とする\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "x = torch.rand(2,3,5) #入力\n",
    "U_q = torch.rand(2,5)\n",
    "U_k = torch.rand(2,5)\n",
    "U_v = torch.rand(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で述べた数式通りに計算を行うと、各batchでの出力は以下の通りとなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch = 0, output: \n",
      " tensor([[1.0679, 1.4628, 0.8342, 1.4374, 1.6284],\n",
      "        [1.0530, 1.4554, 0.8329, 1.4291, 1.6292],\n",
      "        [1.0555, 1.4539, 0.8332, 1.4297, 1.6272]])\n",
      "batch = 1, output: \n",
      " tensor([[1.2303, 1.6044, 0.9414, 1.4426, 1.7949],\n",
      "        [1.2561, 1.6172, 0.9515, 1.4616, 1.8104],\n",
      "        [1.2479, 1.6132, 0.9480, 1.4553, 1.8042]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    x_i = x[i]\n",
    "    q = torch.matmul(x_i,U_q.T)\n",
    "    k = torch.matmul(x_i,U_k.T)\n",
    "    v = torch.matmul(x_i,U_v.T)\n",
    "    omega = q @ k.T\n",
    "    attn = F.softmax(omega / np.sqrt(k.size(1)), dim = 1)\n",
    "    z = attn @ v\n",
    "    print(f\"batch = {i}, output: \\n\",z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のコードのように、for文をbatch_size分だけ回せば理論的には出力が求まりはするが、for文を回すことは計算効率が悪い。そこで、torch.einsumを用いて計算の効率化を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einsumでの計算結果:\n",
      "  tensor([[[1.0679, 1.4628, 0.8342, 1.4374, 1.6284],\n",
      "         [1.0530, 1.4554, 0.8329, 1.4291, 1.6292],\n",
      "         [1.0555, 1.4539, 0.8332, 1.4297, 1.6272]],\n",
      "\n",
      "        [[1.2303, 1.6044, 0.9414, 1.4426, 1.7949],\n",
      "         [1.2561, 1.6172, 0.9515, 1.4616, 1.8104],\n",
      "         [1.2479, 1.6132, 0.9480, 1.4553, 1.8042]]])\n"
     ]
    }
   ],
   "source": [
    "q = torch.einsum(\"ijk,dk->ijd\",[x,U_q])\n",
    "k = torch.einsum(\"ijk,dk->ijd\",[x,U_k])\n",
    "v = torch.einsum(\"ijk,dk->ijd\",[x,U_v])\n",
    "omega = torch.einsum(\"ijk,ilk->ijl\",[q,k])\n",
    "attn = F.softmax(omega / np.sqrt(k.size(2)),dim=2)\n",
    "z = torch.einsum(\"ijk,ikl->ijl\",[attn,v])\n",
    "print(\"einsumでの計算結果:\\n \",z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "einsumを使うことで各バッチでの処理が正しく求められていることがわかる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled Dot Product Self Attentionの実装<br>\n",
    "今回は入力から$Q,K,V$の値を求めるScaled Dot Product Self Attentionの実装について解説を行ってきた。<br>\n",
    "そこで、今回の実装ではforwardに(batch_size, sequence_length, embedding_dim)を入力として想定する**ScaledDotProductSelfAttention**<br>\n",
    "と、forwardに$Q,K,V$の入力を想定する**ScaledDotProductAttention**の2つを実装する。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,Q,K,V):\n",
    "        omega = torch.einsum(\"ijk,ilk->ijl\",[Q,K])\n",
    "        attn = F.softmax(omega / np.sqrt(K.size(2)),dim=2)\n",
    "        z = torch.einsum(\"ijk,ikl->ijl\",[attn,V])\n",
    "        return z\n",
    "class ScaledDotProductSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.U_q = nn.Parameter(torch.tensor(np.random.uniform(low = -np.sqrt(1/d_k),high = np.sqrt(1/d_k),size = (d_k,embed_dim))).float())\n",
    "        self.U_k = nn.Parameter(torch.tensor(np.random.uniform(low = -np.sqrt(1/d_k),high = np.sqrt(1/d_k),size = (d_k,embed_dim))).float())\n",
    "        self.U_v = nn.Parameter(torch.tensor(np.random.uniform(low = -np.sqrt(1/d_v),high = np.sqrt(1/d_v),size = (d_v,embed_dim))).float())\n",
    "    def forward(self,x):\n",
    "        Q = torch.einsum(\"ijk,dk->ijd\",[x,self.U_q])\n",
    "        K = torch.einsum(\"ijk,dk->ijd\",[x,self.U_k])\n",
    "        V = torch.einsum(\"ijk,dk->ijd\",[x,self.U_v])\n",
    "        omega = torch.einsum(\"ijk,ilk->ijl\",[Q,K])\n",
    "        attn = F.softmax(omega / np.sqrt(K.size(2)),dim=2)\n",
    "        z = torch.einsum(\"ijk,ikl->ijl\",[attn,V])\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 5\n",
    "d_k = 2\n",
    "d_v = 5\n",
    "x = torch.rand(batch_size,sequence_length, embedding_dim) #入力\n",
    "sdpa = ScaledDotProductAttention()\n",
    "sdpsa = ScaledDotProductSelfAttention(embedding_dim,d_k,d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つの実装が一致していることを確かめておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductSelfAttentionの実装:\n",
      " tensor([[[-0.1642, -0.1012,  0.0598, -0.0455, -0.3440],\n",
      "         [-0.1647, -0.1027,  0.0615, -0.0467, -0.3450],\n",
      "         [-0.1658, -0.1053,  0.0624, -0.0490, -0.3447]],\n",
      "\n",
      "        [[-0.2318, -0.1854,  0.1680, -0.0798, -0.3056],\n",
      "         [-0.2353, -0.1908,  0.1712, -0.0801, -0.2971],\n",
      "         [-0.2303, -0.1830,  0.1666, -0.0796, -0.3093]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"ScaledDotProductSelfAttentionの実装:\\n\",sdpsa(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductAttentionの実装:\n",
      " tensor([[[-0.1642, -0.1012,  0.0598, -0.0455, -0.3440],\n",
      "         [-0.1647, -0.1027,  0.0615, -0.0467, -0.3450],\n",
      "         [-0.1658, -0.1053,  0.0624, -0.0490, -0.3447]],\n",
      "\n",
      "        [[-0.2318, -0.1854,  0.1680, -0.0798, -0.3056],\n",
      "         [-0.2353, -0.1908,  0.1712, -0.0801, -0.2971],\n",
      "         [-0.2303, -0.1830,  0.1666, -0.0796, -0.3093]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = torch.einsum(\"ijk,dk->ijd\",[x,sdpsa.U_q])\n",
    "K = torch.einsum(\"ijk,dk->ijd\",[x,sdpsa.U_k])\n",
    "V = torch.einsum(\"ijk,dk->ijd\",[x,sdpsa.U_v])\n",
    "print(\"ScaledDotProductAttentionの実装:\\n\",sdpa(Q,K,V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiheadAttentionの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今までで実装してきたScaledDotProductAttentionはTransformerモデルの基本構成要素となっている。<br>\n",
    "しかし、Transformerで採用されているAttentionはScaledDotProductAttentionを並列で実行するMultiheadAttentionというものになっています。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiheadAttentionの理論式は以下の通りとなる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1,...head_h)W^o \\  \\text{where}\\   head_i = \\text{Attention}(Q{W_{i}}^Q,K{W_{i}}^K,V{W_{i}}^V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは、入力した$Q, K, V$それぞれにh個の線形層をかけて、それら全てに$\\text{ScaledDotAttention}$を計算させてできたTensorを結合したものを、更に線形層にいれて最終的な出力を計算するという演算になります。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装上の注意としては、まず最初のh個数の線形層をかけるところは、1つの行列の演算としてまとめたほうが効率が良いです。<br>\n",
    "例えば、$Q{W_{i}}^Q \\ i \\in \\{1,2,3,...,h\\}$の計算では$W^Q = [{W_{1}}^Q, {W_{2}}^Q, {W_{3}}^Q, ... , {W_{h}}^Q] \\ W^Q \\in M_{dim\\_q}\\times_{outdim\\_q \\times num\\_heads}$とおくことで、<br>\n",
    "$QW^Q = [Q{W_{1}}^Q, Q{W_{2}}^Q, Q{W_{3}}^Q, ... , Q{W_{h}}^Q]$とまとめて計算することが可能です。これを今回はnn.Linearで実現します。<br>\n",
    "(上までの実装のようにnn.parameterから制作しても良い)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、$head_i = \\text{Attention}(Q{W_{i}}^Q,K{W_{i}}^K,V{W_{i}}^V)$の実装方法です。ScaledDotProductAttentionを実装したようにeinsumを用いても良いのですが<br>\n",
    "今回は**einops**というライブラリを用いて簡単に計算を行いたいと思います。tensor.splitを使ってheadを分割した後に結合するというやり方もありますか、<br>\n",
    "その実装方針ではメモリ領域の確保などの操作が含まれるため、今回はeinopsでの実装を行います。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, d_k = None, d_v = None):\n",
    "        super().__init__()\n",
    "        self.d_k = embed_dim if d_k == None else d_k\n",
    "        self.d_v = embed_dim if d_v == None else d_v\n",
    "        self.to_q = nn.Linear(embed_dim,self.d_k*num_heads,bias = False)\n",
    "        self.to_k = nn.Linear(embed_dim,self.d_k*num_heads,bias = False)\n",
    "        self.to_v = nn.Linear(embed_dim,self.d_v*num_heads,bias = False)\n",
    "        self.output = nn.Linear(self.d_v*num_heads, embed_dim,bias = False)\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "    def forward(self, x):\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        q = rearrange(q,\"b i (h j)->b h i j\",h=self.num_heads,j=self.d_k)\n",
    "        k = rearrange(k,\"b i (h j)->b h i j\",h=self.num_heads,j=self.d_k)\n",
    "        v = rearrange(v,\"b i (h j)->b h i j\",h=self.num_heads,j=self.d_v)\n",
    "        omega = torch.einsum(\"bhjk,bhlk->bhjl\",[q,k])\n",
    "        attn = self.softmax(omega/np.sqrt(self.d_k))\n",
    "        z = torch.einsum(\"bhjk,bhkl->bhjl\",[attn,v])\n",
    "        concat_z = rearrange(z,\"b h i j->b i (h j)\")\n",
    "        out = self.output(concat_z)\n",
    "        return out, attn\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, d_k = None, d_v = None):\n",
    "        super().__init__()\n",
    "        self.d_k = embed_dim if d_k == None else d_k\n",
    "        self.d_v = embed_dim if d_v == None else d_v\n",
    "        self.to_q = nn.Linear(embed_dim,self.d_k*num_heads,bias = False)\n",
    "        self.to_k = nn.Linear(embed_dim,self.d_k*num_heads,bias = False)\n",
    "        self.to_v = nn.Linear(embed_dim,self.d_v*num_heads,bias = False)\n",
    "        self.output = nn.Linear(self.d_v*num_heads, embed_dim,bias = False)\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "    def forward(self, q,k,v):\n",
    "        q = self.to_q(q)\n",
    "        k = self.to_k(k)\n",
    "        v = self.to_v(v)\n",
    "        q = rearrange(q,\"b i (h j)->b h i j\",h=self.num_heads,j=self.d_k)\n",
    "        k = rearrange(k,\"b i (h j)->b h i j\",h=self.num_heads,j=self.d_k)\n",
    "        v = rearrange(v,\"b i (h j)->b h i j\",h=self.num_heads,j=self.d_v)\n",
    "        omega = torch.einsum(\"bhjk,bhlk->bhjl\",[q,k])\n",
    "        attn = self.softmax(omega/np.sqrt(self.d_k))\n",
    "        z = torch.einsum(\"bhjk,bhkl->bhjl\",[attn,v])\n",
    "        concat_z = rearrange(z,\"b h i j->b i (h j)\")\n",
    "        out = self.output(concat_z)\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "embedding_dim = 5\n",
    "d_k = 2\n",
    "d_v = 5\n",
    "x = torch.rand(batch_size,sequence_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhsa = MultiheadSelfAttention(embed_dim=embedding_dim, num_heads=3,d_k=d_k,d_v=d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1790,  0.3175,  0.1880, -1.7686, -1.2203],\n",
       "          [-0.1723,  0.3033,  0.1802, -1.7113, -1.1644],\n",
       "          [-0.1743,  0.3190,  0.1878, -1.7620, -1.2175]],\n",
       " \n",
       "         [[-0.0847,  0.2050,  0.0675, -1.1218, -0.8401],\n",
       "          [-0.0716,  0.1920,  0.0702, -1.0858, -0.8119],\n",
       "          [-0.0805,  0.2182,  0.0610, -1.1137, -0.8357]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[[[0.3714, 0.2822, 0.3464],\n",
       "           [0.4062, 0.2386, 0.3552],\n",
       "           [0.3734, 0.2762, 0.3504]],\n",
       " \n",
       "          [[0.3410, 0.3252, 0.3338],\n",
       "           [0.3442, 0.3100, 0.3459],\n",
       "           [0.3370, 0.3236, 0.3394]],\n",
       " \n",
       "          [[0.4132, 0.2586, 0.3282],\n",
       "           [0.5302, 0.1878, 0.2820],\n",
       "           [0.4177, 0.2666, 0.3157]]],\n",
       " \n",
       " \n",
       "         [[[0.3453, 0.3432, 0.3115],\n",
       "           [0.3543, 0.3521, 0.2936],\n",
       "           [0.3505, 0.3386, 0.3109]],\n",
       " \n",
       "          [[0.3345, 0.3027, 0.3628],\n",
       "           [0.3343, 0.3020, 0.3637],\n",
       "           [0.3335, 0.3245, 0.3420]],\n",
       " \n",
       "          [[0.3484, 0.3033, 0.3483],\n",
       "           [0.3643, 0.2605, 0.3753],\n",
       "           [0.3526, 0.2971, 0.3503]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhsa(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析デモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制作したScaledDotProductAttentionとMultiheadAttentionを用いて簡単な分析を行ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#openpyxlが入っていないとバグるので注意\n",
    "dataset = load_dataset(\"snow_simplified_japanese_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.permutation(50000)\n",
    "train_index = index[:45000]\n",
    "test_index = index[45000:]\n",
    "#何だか変な実装になってしまいましたが、訓練データを45000, テストデータを5000として分割を行う。\n",
    "ja_train = np.array(dataset[\"train\"][\"simplified_ja\"])[train_index].tolist()\n",
    "ja_test = np.array(dataset[\"train\"][\"simplified_ja\"])[test_index].tolist()\n",
    "en_train = np.array(dataset[\"train\"][\"original_en\"])[train_index].tolist()\n",
    "en_test = np.array(dataset[\"train\"][\"original_en\"])[test_index].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語処理では分析の前に文章を分かち書きし、一意な単語を整数にマッピングする必要があります。<br>\n",
    "今回扱うデータは眺めているとわかるのですが、顔文字などの特殊文字のない、とても癖のないきれいなデータであることがわかります。<br>\n",
    "そのため、データをすぐに分かち書きしても良いでしょう。<br>\n",
    "(実際のデータに顔文字や特殊文字、htmlの記法などが含まれていた場合、それらの文字には欠損値のように特別な処理を施す必要があります。)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語の分かち書きにはjanome, 英語の分かち書きにはtorchtext.data.utils.get_tokenizerを用います。<br>\n",
    "では、まずは一意な単語を数字に変換するtorchtext.vocab.vocabオブジェクトを製作しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import vocab\n",
    "def create_ja_vocab(*args):\n",
    "    ja_tokenizer = Tokenizer()\n",
    "    ja_token_count = Counter()\n",
    "    for data in args:\n",
    "        for d in tqdm(data):\n",
    "            tokens = list(ja_tokenizer.tokenize(d,wakati=True))\n",
    "            ja_token_count.update(tokens)\n",
    "    ordered_dict = OrderedDict(sorted(ja_token_count.items(),key = lambda x: x[1], reverse = True))\n",
    "    ja_vocab = vocab(ordered_dict)\n",
    "    ja_vocab.insert_token(\"<pad>\", 0)\n",
    "    ja_vocab.insert_token(\"<unk>\", 1)\n",
    "    ja_vocab.insert_token(\"<bos>\", 2)\n",
    "    ja_vocab.insert_token(\"<eos>\", 3)\n",
    "    ja_vocab.set_default_index(1)\n",
    "    return ja_vocab\n",
    "def create_en_vocab(*args):\n",
    "    en_tokenizer = get_tokenizer(\"basic_english\")\n",
    "    en_token_count = Counter()\n",
    "    for data in args:\n",
    "        for d in tqdm(data):\n",
    "            tokens = en_tokenizer(d)\n",
    "            en_token_count.update(tokens)\n",
    "    ordered_dict = OrderedDict(sorted(en_token_count.items(),key = lambda x: x[1], reverse = True))\n",
    "    en_vocab = vocab(ordered_dict)\n",
    "    en_vocab.insert_token(\"<pad>\", 0)\n",
    "    en_vocab.insert_token(\"<unk>\", 1)\n",
    "    en_vocab.insert_token(\"<bos>\", 2)\n",
    "    en_vocab.insert_token(\"<eos>\", 3)\n",
    "    en_vocab.set_default_index(1)\n",
    "    return en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45000/45000 [00:14<00:00, 3181.77it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3287.01it/s]\n",
      "100%|██████████| 45000/45000 [00:00<00:00, 299098.92it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 272258.40it/s]\n"
     ]
    }
   ],
   "source": [
    "ja_vocab = create_ja_vocab(ja_train,ja_test)\n",
    "en_vocab = create_en_vocab(en_train,en_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例文を打ってみて、製作したvocabオブジェクトがどのように動作するかを試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語のvocab:  [75, 5, 1741, 19, 4]\n",
      "日本語のvocabの語彙サイズ: 3919\n",
      "英語のvocab:  [22, 9, 73, 1788, 4]\n",
      "英語のvocabの語彙サイズ: 6626\n"
     ]
    }
   ],
   "source": [
    "print(\"日本語のvocab: \",ja_vocab([\"これ\", \"は\", \"例\", \"です\", \"。\"]))\n",
    "print(\"日本語のvocabの語彙サイズ:\", len(ja_vocab))\n",
    "print(\"英語のvocab: \",en_vocab([\"this\", \"is\", \"an\", \"example\", \".\"]))\n",
    "print(\"英語のvocabの語彙サイズ:\", len(en_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にデータセットを定義します。最終的には日本語のstring型を入力して英語のstring型を出力して欲しいため、データにおいてもstring型のデータを入力に用いるように構築します。<br>\n",
    "カスタムデータセットの構築方法はこのリポジトリのpytorch_command.ipynbでも解説しています。<br>\n",
    "もし学習が不十分なときは、見返しながらやると良いでしょう。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はカスタムデータセットを用いてデータセットを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class sentence_dataset(Dataset):\n",
    "    def __init__(self, ja, en):\n",
    "        if len(ja) != len(en):\n",
    "            raise ValueError(\"len(ja) != len(en)\")\n",
    "        self.ja = ja\n",
    "        self.en = en\n",
    "    def __getitem__(self, index):\n",
    "        return self.ja[index], self.en[index]\n",
    "    def __len__(self):\n",
    "        return len(self.ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sentence_dataset(ja_train, en_train)\n",
    "test_dataset = sentence_dataset(ja_test, en_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、データの処理関数を記述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2tensor(batch):\n",
    "    ja_tokenizer = Tokenizer()\n",
    "    en_tokenizer = get_tokenizer(\"basic_english\")\n",
    "    text2int_ja = lambda x: [ja_vocab[token] for token in list(ja_tokenizer.tokenize(x,wakati=True))]\n",
    "    text2int_en = lambda x: [en_vocab[\"<bos>\"]]+[en_vocab[token] for token in en_tokenizer(x)]+[en_vocab[\"<eos>\"]]\n",
    "    text_ja, text_en, length_ja, length_en = [], [], [], []\n",
    "    for ja, en in batch:\n",
    "        processed_ja = torch.tensor(text2int_ja(ja), dtype=torch.int64)\n",
    "        processed_en = torch.tensor(text2int_en(en), dtype=torch.int64)\n",
    "        text_ja.append(processed_ja)\n",
    "        text_en.append(processed_en)\n",
    "        length_ja.append(processed_ja.size(0))\n",
    "        length_en.append(processed_en.size(0))\n",
    "    length_ja = torch.tensor(length_ja)\n",
    "    length_en = torch.tensor(length_en)\n",
    "    padded_text_ja = nn.utils.rnn.pad_sequence(text_ja, batch_first=True) #系列帳を揃える。\n",
    "    padded_text_en = nn.utils.rnn.pad_sequence(text_en, batch_first=True)\n",
    "    return padded_text_ja, padded_text_en, length_ja, length_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderを定義します。定義する際、訓練セットはShuffleをTrueに、collate_fn引数に製作した処理関数をいれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 100\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=str2tensor)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=str2tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上手く動作しているかをtest_dlの最初のデータで調べてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Japanese Sentence:\n",
      " 外は寒いよ。コートを着なさい。\n",
      "Data Japanese: \n",
      " tensor([172,   5, 663,  48,   4, 746,   8, 575,  76,   4,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0]) torch.Size([20])\n",
      "Data English Sentence:\n",
      " it is cold outdoors . put on your coat .\n",
      "Data English: \n",
      " tensor([   2,   15,    9,  175, 3170,    4,  143,   32,   37,  662,    4,    3,\n",
      "           0,    0,    0,    0,    0]) torch.Size([17])\n",
      "Data Japanese Sentence:\n",
      " 外は寒いよ。コートを着なさい。\n",
      "Data Japanese: \n",
      " tensor([ 12,   9,  54,   5, 657,   4,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0]) torch.Size([17])\n",
      "Data English Sentence:\n",
      " it is cold outdoors . put on your coat .\n",
      "Data English: \n",
      " tensor([  2,  19, 117,   9, 386,   4,   3,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0]) torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for ja, en, len_ja, len_en in test_dl:\n",
    "    print(\"Data Japanese Sentence:\\n\", ja_test[0])\n",
    "    print(\"Data Japanese: \\n\",ja[0],ja[0].shape)\n",
    "    print(\"Data English Sentence:\\n\", en_test[0])\n",
    "    print(\"Data English: \\n\",en[0],en[0].shape)\n",
    "    if i == 1:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では簡単に学習を行ってみよう。今回はMultiheadAttentionに2つの層を通して文章を生成することを考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ja_vocab_size = len(ja_vocab)\n",
    "en_vocab_size = len(en_vocab)\n",
    "embedding_dim = 300\n",
    "\n",
    "class ja2en_multiheadattention(nn.Module):\n",
    "    def __init__(self, ja_vocab_size, en_vocab_size, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_ja = nn.Embedding(ja_vocab_size, embedding_dim)\n",
    "        self.embedding_en = nn.Embedding(en_vocab_size, embedding_dim)\n",
    "        self.encoder = MultiheadAttention(embedding_dim,num_heads)\n",
    "        self.decoder = MultiheadAttention(embedding_dim,num_heads)\n",
    "        self.to_out = nn.Linear(embedding_dim, en_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "    def forward(self,src, tgt):\n",
    "        src = self.embedding_ja(src)\n",
    "        tgt = self.embedding_en(tgt)\n",
    "        src,_ = self.encoder(src,src,src)\n",
    "        x,_ = self.decoder(tgt, src, src)\n",
    "        x = self.to_out(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.metrics import bleu_score\n",
    "model = ja2en_multiheadattention(ja_vocab_size, en_vocab_size, embedding_dim, num_heads=6).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のコードは一度実行したら実行しないほうがいいです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005145474457588678\n",
      "Train ACCURACY:  0.38860458802769104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.97it/s]\n",
      "  5%|▌         | 1/20 [00:40<12:56, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.005125669614115818\n",
      "Test ACCURACY:  0.3909987819732034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005133563069622142\n",
      "Train ACCURACY:  0.39091673447247083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.88it/s]\n",
      " 10%|█         | 2/20 [01:21<12:11, 40.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005137640372420238\n",
      "Train ACCURACY:  0.39042073832790447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.82it/s]\n",
      " 15%|█▌        | 3/20 [02:01<11:29, 40.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.00514738424678468\n",
      "Train ACCURACY:  0.389343303874915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.98it/s]\n",
      " 20%|██        | 4/20 [02:42<10:48, 40.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005116075126644855\n",
      "Train ACCURACY:  0.3928105988914425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.68it/s]\n",
      " 25%|██▌       | 5/20 [03:23<10:09, 40.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005133195431665059\n",
      "Train ACCURACY:  0.39091673447247083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.92it/s]\n",
      " 30%|███       | 6/20 [04:03<09:28, 40.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005146625617588908\n",
      "Train ACCURACY:  0.389426318651441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.87it/s]\n",
      " 35%|███▌      | 7/20 [04:44<08:47, 40.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005127997621257944\n",
      "Train ACCURACY:  0.3914943774556293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.84it/s]\n",
      " 40%|████      | 8/20 [05:24<08:07, 40.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.00512566793391144\n",
      "Train ACCURACY:  0.3917416034669556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 15.00it/s]\n",
      " 45%|████▌     | 9/20 [06:05<07:26, 40.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005142923415217318\n",
      "Train ACCURACY:  0.38984105420459175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.89it/s]\n",
      " 50%|█████     | 10/20 [06:46<06:45, 40.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:37<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.0051302658816798005\n",
      "Train ACCURACY:  0.39124695039306046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.94it/s]\n",
      " 55%|█████▌    | 11/20 [07:27<06:07, 40.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:42<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005122702657434188\n",
      "Train ACCURACY:  0.3920709258256632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.41it/s]\n",
      " 60%|██████    | 12/20 [08:13<05:39, 42.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:39<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005114826692698984\n",
      "Train ACCURACY:  0.39245033112582783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.24it/s]\n",
      " 65%|██████▌   | 13/20 [08:56<04:57, 42.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:39<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.0051595812068678795\n",
      "Train ACCURACY:  0.3880119907344325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.57it/s]\n",
      " 70%|███████   | 14/20 [09:38<04:15, 42.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:41<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.00512567844447697\n",
      "Train ACCURACY:  0.3917416034669556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.37it/s]\n",
      " 75%|███████▌  | 15/20 [10:24<03:36, 43.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:41<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.0051280416755588405\n",
      "Train ACCURACY:  0.3914943774556293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.42it/s]\n",
      " 80%|████████  | 16/20 [11:09<02:55, 43.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:41<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005126552288567457\n",
      "Train ACCURACY:  0.39165921712041174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.49it/s]\n",
      " 85%|████████▌ | 17/20 [11:54<02:12, 44.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:41<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005127255202825397\n",
      "Train ACCURACY:  0.39157680845299375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.42it/s]\n",
      " 90%|█████████ | 18/20 [12:39<01:29, 44.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:41<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005128663839686531\n",
      "Train ACCURACY:  0.3914119241192412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.52it/s]\n",
      " 95%|█████████▌| 19/20 [13:24<00:44, 44.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:41<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CrossEntropyLoss:  0.005134676579088862\n",
      "Train ACCURACY:  0.39075149213239285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.45it/s]\n",
      "100%|██████████| 20/20 [14:09<00:00, 42.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CrossEntropyLoss:  0.0051227686088645644\n",
      "Test ACCURACY:  0.39115712545676007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "model.train()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    all_length = 0\n",
    "    all_acc = 0\n",
    "    all_loss = 0\n",
    "    for japanese, english, _, _ in tqdm(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        japanese = japanese.to(device)\n",
    "        english = english.to(device)\n",
    "        pred = model(japanese, english)\n",
    "        pred = rearrange(pred,\"i j k->(i j) k\")\n",
    "        tgt = F.one_hot(english, num_classes=en_vocab_size)\n",
    "        tgt = rearrange(tgt,\"i j k->(i j) k\")\n",
    "        loss = criterion(pred, tgt.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_acc += (pred.argmax(dim = -1) == tgt.argmax(dim = -1)).sum().item()\n",
    "        all_length += len(pred)\n",
    "        all_loss += loss.item()\n",
    "    print(\"Train CrossEntropyLoss: \", all_loss / all_length)\n",
    "    print(\"Train ACCURACY: \", all_acc / all_length)\n",
    "    model.eval()\n",
    "    all_length = 0\n",
    "    all_acc = 0\n",
    "    all_loss = 0\n",
    "    for japanese, english, _, _ in tqdm(test_dl):\n",
    "        japanese = japanese.to(device)\n",
    "        english = english.to(device)\n",
    "        pred = model(japanese, english)\n",
    "        pred = rearrange(pred,\"i j k->(i j) k\")\n",
    "        tgt = F.one_hot(english, num_classes=en_vocab_size)\n",
    "        tgt = rearrange(tgt,\"i j k->(i j) k\")\n",
    "        loss = criterion(pred, tgt.float())\n",
    "        all_acc += (pred.argmax(dim = -1) == tgt.argmax(dim = -1)).sum().item()\n",
    "        all_length += len(pred)\n",
    "        all_loss += loss.item()\n",
    "    print(\"Test CrossEntropyLoss: \", all_loss / all_length)\n",
    "    print(\"Test ACCURACY: \", all_acc / all_length)\n",
    "    if best_acc < all_acc / all_length:\n",
    "        best_acc = all_acc / all_length\n",
    "        best_params = model.parameters\n",
    "    #cuda: 1 epoch 0:39, batch_size=100,  embedding_dim = 300\n",
    "    #cpu:  1 epoch 3:34, batch_size=100,  embedding_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルのパラメーターは保存しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters = best_params\n",
    "torch.save(model.state_dict(), \"naive_attention.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練結果はどうでしたか？思ったよりも結果がでなかったではないでしょうか？<br>\n",
    "これは、RNNベースのSeq2SeqモデルからAttentionモデルに変更したことによるモデルアーキテクチャの違いや、<br>\n",
    "ただ愚直にMultiheadAttentionを実装してしまったことが原因となっています。<br>\n",
    "先に実験した愚直なMultiheadAttentionモデルのアーキテクチャを改善したものがかの有名なTransformerとなっています。<br>\n",
    "では上のMultiheadAttentionを改善する形で実装をしていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今までに制作したMultiheadAttentionとTransformerは全てtorch.nnのモジュールとして提供されています。実際にTransformerより上位のモデルを構築する際はそちらを用いたほうが開発効率が良いでしょう。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorchのライブラリ目次<br>\n",
    "[torch.nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)<br>\n",
    "[torch.nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
