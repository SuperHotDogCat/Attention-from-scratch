{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このファイルではGPT from scratchで学習したモデルの改善を目標とする。このファイルの立ち位置は同リポジトリに含まれている\n",
    "- pytorch_command.ipynb\n",
    "- attention_from_scratch.ipynb\n",
    "- GPT_from_scratch.ipynb<br>\n",
    "の次に読むことを想定されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA環境が壊れていないことを祈りながら確認->  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "print(\"CUDA環境が壊れていないことを祈りながら確認-> \", torch.cuda.is_available())\n",
    "import subprocess\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreLN\n",
    "GPT from scratchでは、学習中に勾配が変化しない状態がいつまで経っても続く時があった。<br>\n",
    "これはLayer Normalization層がMultiheadAttentionやFFN層の後に置かれるというPostLNの形をとっているからであると最近では言われている<br>\n",
    "このため、改善点の一つとして、サブレイヤーの前にLayer Normalization層を置くPreLNを行う。<br>\n",
    "最近のGPTモデルではPreLNを行なっているようである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLNGPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0., layer_eps=1e-05, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.maskedmultiheadattention = nn.MultiheadAttention(embedding_dim, num_heads,batch_first=batch_first)\n",
    "        self.dropout_selfattn = nn.Dropout(p = drop_out_rate)\n",
    "        self.layernorm_selfattn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, embedding_dim))#GELUに変更\n",
    "        self.layernorm_ffn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "        self.dropout_ffn = nn.Dropout(p = drop_out_rate)\n",
    "\n",
    "    def forward(self, x, pad_mask_self = None, mask_self=None):\n",
    "        #PreLNにする\n",
    "        dx = self.layernorm_selfattn(x)\n",
    "\n",
    "        dx, _ = self.maskedmultiheadattention(dx,dx,dx,key_padding_mask = pad_mask_self, attn_mask = mask_self)\n",
    "\n",
    "        dx = self.dropout_selfattn(dx)\n",
    "\n",
    "        x = x+dx\n",
    "\n",
    "        dx = self.layernorm_ffn(x)\n",
    "\n",
    "        dx = self.dropout_ffn(self.ffn(dx))\n",
    "\n",
    "        x = x + dx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改善したGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このPreLNGPTDecoderLayerを用いてGPTモデルを制作する。ところで、今回はGPTのgenerate_sentence関数でもtemperatureとtopKという手法を追加した。<br>\n",
    "これにより多様性のある文章が生成される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature: Tはトークンの予測確率$p_{i}$を以下のように変換するパラメーターである。<br>\n",
    "$$\n",
    "p_{i} \\to \\dfrac{\\exp(\\dfrac{p_i}{T})}{\\sum_{j}\\exp(\\dfrac{p_j}{T})}\n",
    "$$\n",
    "Tを大きくすればするほどもともとの確率が低いトークンの確率が高くなる。<br>\n",
    "topKは予測確率の一番高いトークンを選ぶのではなく、予測確率の高いトークンのうち、上位K個を出力対象としてランダムに選ぶ手法である。<br>\n",
    "これらの手法を組み合わせるにより、出力は毎回ランダムであるが、多様性のある文章が生成されるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0.,\\\n",
    "                  layer_eps=1e-05, batch_first = False, T = 10000, N = 1):\n",
    "        super().__init__()\n",
    "        #Tはmax_lenを表している\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,)\n",
    "        self.positional_embedding = nn.Embedding(T, embedding_dim)\n",
    "        self.decoder = nn.ModuleList([PreLNGPTDecoderLayer(embedding_dim, ffn_dim, num_heads, drop_out_rate,\\\n",
    "                                                               layer_eps, batch_first) for _ in range(N)])\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size, bias = False)\n",
    "        self.vocab_size = vocab_size\n",
    "    def forward(self, x, y = None,pad_mask_self = None, mask_self=None):\n",
    "        \"\"\"\n",
    "        yはxを1つだけずらしたデータである\n",
    "        x = data[a:b]なら、y = data[a+1:b+1]となる。\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        pos = torch.arange(0,x.size(1),dtype=torch.long).unsqueeze(0).to(x.device)\n",
    "        pos = self.positional_embedding(pos)\n",
    "        x = x + pos\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, pad_mask_self = pad_mask_self, mask_self = mask_self)\n",
    "        x = self.linear(x)\n",
    "        if y != None:\n",
    "            loss = F.cross_entropy(x.view(-1, x.size(-1)), y.view(-1), ignore_index=-1) \n",
    "            #ignore_index=-1はyをonehotベクトル化しないでcross_entropyを使うために使用\n",
    "            pred = x.argmax(dim = -1).detach().cpu()\n",
    "            return loss,pred\n",
    "        loss = None\n",
    "        pred = x[:,[-1],:]\n",
    "        return loss, pred\n",
    "    def create_mask(self, x: torch.tensor, x_pad: int, device: str):\n",
    "        \"\"\"\n",
    "        (batch_size, sequence_length, embedding_dim)の入力を想定\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Trueが無視される値であることに注意すること\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        #srcのマスク制作\n",
    "        padding_mask = (x == x_pad)\n",
    "        mask = torch.triu(torch.ones(size = (seq_len, seq_len))==1).transpose(0,1) #下三角行列を作る\n",
    "        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask==1.,float(0.0)).to(device)\n",
    "        return padding_mask, mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,bos: str, sentence_size, tokenizer, device):\n",
    "        self.eval()\n",
    "        bos_tokenized = tokenizer.encode_ordinary(bos)\n",
    "        bos_tokenized = bos_tokenized[-sentence_size:]\n",
    "        bos_tokenized = torch.LongTensor([bos_tokenized])\n",
    "        _, add_sentence = self(bos_tokenized.to(device))\n",
    "        self.train()\n",
    "        return add_sentence\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_sentence(self, bos: str, sentence_size, generate_tokens, tokenizer, device, top_K = None, temperature = 1.0):\n",
    "        return_sentence = bos\n",
    "        for i in range(generate_tokens):\n",
    "            add_sentence = self.generate(return_sentence, sentence_size, tokenizer,device)\n",
    "            add_sentence = add_sentence[:,-1,:] / temperature #(1, vocab_size)\n",
    "            if top_K is not None:\n",
    "                v, _ = torch.topk(add_sentence, min(top_K, add_sentence.size(-1)))\n",
    "                #v[:, [-1]]がtopkの中でも最小値を取る。これより小さいやつは予想に含めない。\n",
    "                add_sentence[add_sentence < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(add_sentence, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            return_sentence += tokenizer.decode_batch(idx_next.tolist())[0]\n",
    "        return return_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは訓練に移ろう、使うデータはGPT＿from＿scratchでも製作したデータである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap(\"bin/train.bin\", dtype = np.uint16, mode = \"r\")\n",
    "val_data = np.memmap(\"bin/val.bin\", dtype = np.uint16, mode = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size = 1024\n",
    "batch_size = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def get_batch(split: str, batch_size = 256,device = \"cpu\")->torch.Tensor:\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - sentence_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+sentence_size]).astype(np.int64)) for i in index])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+sentence_size]).astype(np.int64)) for i in index])\n",
    "    if device == \"cuda\":\n",
    "        return x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_size = 768\n",
    "num_heads = 6\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "#KarpathyのminGPTを参考に、パラメーターを設定した。\n",
    "depth = 6 \n",
    "gpt = GPT(50257, embedding_size, embedding_size*4, num_heads, 0, batch_first=True, T = sentence_size, N = depth).to(device) \n",
    "#事前学習のときはDropout無し、ファインチューニングのときはありが好ましい\n",
    "warmup_iters = 2000\n",
    "\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cos scheduler関数は少し変更を加え、clipメソッドを用いてmax_lrとmin_lrの間で必ず学習率が出力されるようにした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 2.5e-5\n",
    "min_lr = 2.5e-6\n",
    "max_iters = 10000\n",
    "def get_lr(cur_iter):\n",
    "    #cur_iter現在のiteration\n",
    "    if cur_iter < warmup_iters:\n",
    "        return max_lr * cur_iter / warmup_iters\n",
    "    return (max_lr * (np.cos(cur_iter / max_iters * np.pi) + 1)).clip(min_lr, max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の際もただパラメーターを保管するだけではなく、パラメーター、最も良い検証データの損失、その時のiterationを記録するようにした。<br>\n",
    "LLMは学習中に突然損失が大きくなるときがあるので、損失がNaNとなったときは学習を停止し、その時のパラメーターは保存しないようにした。<br>\n",
    "こうしないとパラメーターの一部が知らぬ間にNaNとなっている場合がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'最初の訓練時のコードでは以下のように初期化を行なう。\\nimport gc\\nfrom tqdm import tqdm\\nbatch_iteration = 128\\nscaler = torch.cuda.amp.GradScaler(enabled=True)\\nbest_loss = 1e9\\nbegin = 0\\nval_iteration = 1\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"最初の訓練時のコードでは以下のように初期化を行なう。\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "batch_iteration = 128\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "best_loss = 1e9\n",
    "begin = 0\n",
    "val_iteration = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "checkpoint = torch.load(\"best_checkpoint.bin\", map_location=\"cpu\") #チェックポイントがあるなら使う\n",
    "batch_iteration = 256\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True) \n",
    "best_loss = checkpoint[\"best_loss\"]\n",
    "begin = checkpoint[\"iter\"]\n",
    "val_iteration = 10\n",
    "gpt.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "del checkpoint\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "sleep(5)\n",
    "gpt.train()\n",
    "for cur_iter in tqdm(range(begin,max_iters)):\n",
    "    optimizer.lr = get_lr(cur_iter+1)\n",
    "    for batch_iter in range(batch_iteration):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            x,y = get_batch(\"train\",batch_size=batch_size,device=device)\n",
    "            padding_mask, mask = gpt.create_mask(x, 0, device)\n",
    "            loss, pred = gpt(x,y,padding_mask,mask)\n",
    "        scaler.scale(loss).backward() \n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "        del x, y\n",
    "        del padding_mask, mask\n",
    "        del loss\n",
    "        del pred\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    valid_loss = 0\n",
    "    for val_iter in range(val_iteration):\n",
    "        with torch.no_grad(): #こうしないとCUDAERRORが起きる\n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                x,y = get_batch(\"valid\",batch_size=batch_size,device=device)\n",
    "                padding_mask, mask = gpt.create_mask(x, 0, device)\n",
    "                loss, pred = gpt(x,y,padding_mask,mask)\n",
    "                valid_loss += loss.detach()\n",
    "                del loss\n",
    "                del x, y\n",
    "                del padding_mask, mask\n",
    "                del pred\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "    if best_loss > valid_loss.item() / val_iteration:\n",
    "        best_loss = valid_loss.item() / val_iteration\n",
    "        checkpoint = {\n",
    "            \"model\": gpt.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler,\n",
    "            \"iter\": cur_iter,\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, \"best_checkpoint.bin\")\n",
    "        print(\"params updated. BestLoss: \", best_loss)\n",
    "        print(\"Val all loss\", valid_loss.item())\n",
    "        with open(\"learning_detail.txt\",\"w\") as f:\n",
    "            f.write(\"学習状況\\n\")\n",
    "            f.write(f\"iter: {cur_iter}\\n\")\n",
    "            f.write(f\"hyper params: \\n\")\n",
    "            f.write(f\"vocab_size: 50257, embedding size: {embedding_size}, ffn: {embedding_size*4}, num_heads: {num_heads}, Depth: {depth}, sentnce_size: {sentence_size}\\n\")\n",
    "            f.write(f\"lr: {optimizer.lr},best_loss: {best_loss}\\n\")\n",
    "            f.close()\n",
    "    if torch.isnan(valid_loss):\n",
    "        print(\"Loss is NaN!\")\n",
    "        break\n",
    "    checkpoint = {\n",
    "    \"model\": gpt.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"scaler\": scaler,\n",
    "    \"iter\": cur_iter,\n",
    "    \"best_loss\": best_loss,\n",
    "    \"loss\": valid_loss.item()\n",
    "    }\n",
    "    torch.save(checkpoint, \"latest_checkpoint.bin\")\n",
    "    with open(\"learning_detail_latest.txt\",\"w\") as f:\n",
    "        f.write(\"学習状況\\n\")\n",
    "        f.write(f\"iter: {cur_iter}\\n\")\n",
    "        f.write(f\"hyper params: \\n\")\n",
    "        f.write(f\"vocab_size: 50257, embedding size: {embedding_size}, ffn: {embedding_size*4}, num_heads: {num_heads}, Depth: {depth}, sentnce_size: {sentence_size}\\n\")\n",
    "        f.write(f\"lr: {optimizer.lr},best_loss: {best_loss}\\n\")\n",
    "        f.write(f\"val_loss: {valid_loss.item() / val_iteration}\\n\")\n",
    "        f.close()\n",
    "        del valid_loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードを何日間かの間、リポジトリ製作者のパソコンで実行した結果のなかで、<br>\n",
    "最もマシと考えられる結果が得られたときのパラメーターを用いて出力を見てみることにする。<br>\n",
    "このチェックポイントファイルは1.4GB以上あるのでGithub上にあげることができないのはお許しください。<br>\n",
    "このときの学習状況は以下の通りとなった。<br>\n",
    "学習状況<br>\n",
    "iter: 325/10000<br>\n",
    "hyper params: <br>\n",
    "vocab_size: 50257, embedding size: 768, ffn: 3072, num_heads: 6, Depth: 6, sentnce_size: 1024<br>\n",
    "lr: 4.075000000000001e-06, best_loss: 3.849387884140015<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"best_checkpoint.bin\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He has two sisters, and ents, but not a great-uncle, The Boy in all that is the same type-snow, both, but a good name. I know there's two very bad and they are all but a good reason they've probably got good in comparison, in fact I know that they haven't the only the most of these guys in our division of the first names.\n",
      "\n",
      "'The same old guys; I'm not the opposite that this is that, either (although they just got better guys are that, in that team. You can probably have more of their name: they probably just the same types as those other names and\n"
     ]
    }
   ],
   "source": [
    "print(gpt.generate_sentence(\"He has two sisters, and \", \\\n",
    "                            sentence_size, 128, tokenizer,device,top_K=20,temperature = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I always have a breakfast. She just isn�t going to go in the bed. If someone wants a couple or don�t even care to make it to go into hospital the way, then some kid to sleep. I am sick, but we are a good time for my mom and they're so happy that we can�t need a little bit sleep, she�s so you know she needs. The fact that it�t. I have a big sleep to keep me sleep well so that a good day, the baby that she loves a better job, the only way to get around, they�t. My mom.<|endoftext|> is a sleep out\n"
     ]
    }
   ],
   "source": [
    "print(gpt.generate_sentence(\"I always have a breakfast.\", \\\n",
    "                            sentence_size, 128, tokenizer,device,top_K=20,temperature = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan is a country, which has the longest-populifying population of about 10 people, but most recently. But this trend has seen a trend in the global poverty. A drop by population of 10 is emerging growth since 2010. What are people have seen some of the trend that is now is rising across all the global food, but more of which has seen some is experiencing growth trends in recent years, and growing. For most is a percentage since then growth since 2010. Now that growth since 2006 and growing by growth means increasing growth over the over half is growing more than this increase is just one, in part, with over 10, a trend in 2007 is\n"
     ]
    }
   ],
   "source": [
    "print(gpt.generate_sentence(\"Japan is a country, which\", \\\n",
    "                            sentence_size, 128, tokenizer,device,top_K=20,temperature = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT_from_scratch.ipynbよりかはまともな出力が得られたと考えられる。実際にPreLNに変えたことで学習はいくらか向上したようである。<br>\n",
    "今回製作したGPTは最も初代のGPT1であるため、タスクに適合させるためには、<br>\n",
    "転移学習を行なう必要がある(いわゆるZero shot，Few shotを意図して作られてはいない)(詳しくは[https://data-analytics.fun/2020/04/18/understanding-openai-gpt/](https://data-analytics.fun/2020/04/18/understanding-openai-gpt/)などが詳しい)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでも制作に当たり反省点がいくらかある。それを述べていこう。<br>\n",
    "・10000 epoch学習させるつもりだったが、325iterでの結果が最も良かった。<br><br>\n",
    "これは学習のバッチサイズが6と非常に小さく、一回のパラメーター更新で6個の文章を用いてしか<br>\n",
    "パラメーターを更新することができなかったことが原因と思われる<br>\n",
    "実は、学習が完了してから勾配累積という方法を知り、それを用いて擬似的にバッチサイズを増やすことができることを学んだ。<br>\n",
    "次に大規模な学習を行なうときはこの手法を最初から用いたほうが良い。\n",
    "\n",
    "・モデルのサイズを小さくしてしまった。<br><br>\n",
    "モデルのサイズがGPUの関係で小さくなってしまった。次やるときは社会人になって80GBぐらいのGPU買ってやるしかない。<br>\n",
    "\n",
    "・さらに自然な出力に向けて<br><br>\n",
    "\n",
    "GPT2, GPT3のようにタスクをこなせるようにFew shotの実現ができるようなデータでも学習を行いたい。<br>\n",
    "Reinfrce Learning from Human Feedbackのようにまだまだ改善できる手法は残っているのでこれらの実装を行っていきたい<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でこのAttention_from_scratchリポジトリで提供する全コンテンツの解説を終了したいと思います。<br>\n",
    "はじめはAttentionを1から制作するリポジトリを作るという意気込みで始めましたが、最近の深層自然言語処理をまとめたようなリポジトリになってしまったなと思います。<br>\n",
    "\n",
    "もしこのリポジトリが役に立つ人がいらっしゃいましたら非常に光栄です。<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e2ef60554f914b7f3190499c85ea0c48ae4fc01e8f403cc646d16228abbf679"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
