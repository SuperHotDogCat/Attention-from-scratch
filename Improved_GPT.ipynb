{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このファイルではGPT from scratchで学習したモデルの改善を目標とする。このファイルの立ち位置は同リポジトリに含まれている\n",
    "- pytorch_command.ipynb\n",
    "- attention_from_scratch.ipynb\n",
    "- GPT_from_scratch.ipynb<br>\n",
    "の次に読むことを想定されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA環境が壊れていないことを祈りながら確認-> True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "print(\"CUDA環境が壊れていないことを祈りながら確認-> \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreLN\n",
    "GPT from scratchでは、学習中に勾配が変化しない状態がいつまで経っても続く時があった。<br>\n",
    "これはLayer Normalization層がMultiheadAttentionやFFN層の後に置かれるというPostLNの形をとっているからであると最近では言われている<br>\n",
    "このため、改善点の一つとして、サブレイヤーの前にLayer Normalization層を置くPreLNを行う。<br>\n",
    "最近のGPTモデルではPreLNを行なっているようである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLNGPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0., layer_eps=1e-05, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.maskedmultiheadattention = nn.MultiheadAttention(embedding_dim, num_heads,batch_first=batch_first)\n",
    "        self.dropout_selfattn = nn.Dropout(p = drop_out_rate)\n",
    "        self.layernorm_selfattn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, embedding_dim))#GELUに変更\n",
    "        self.layernorm_ffn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "        self.dropout_ffn = nn.Dropout(p = drop_out_rate)\n",
    "\n",
    "    def forward(self, x, pad_mask_self = None, mask_self=None):\n",
    "        #PreLNにする\n",
    "        dx = self.layernorm_selfattn(x)\n",
    "\n",
    "        dx, _ = self.maskedmultiheadattention(dx,dx,dx,key_padding_mask = pad_mask_self, attn_mask = mask_self)\n",
    "\n",
    "        dx = self.dropout_selfattn(dx)\n",
    "\n",
    "        x = x+dx\n",
    "\n",
    "        dx = self.layernorm_ffn(x)\n",
    "\n",
    "        dx = self.dropout_ffn(self.ffn(dx))\n",
    "\n",
    "        x = x + dx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改善したGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このPreLNGPTDecoderLayerを用いてGPTモデルを制作する。ところで、今回はGPTのgenerate_sentence関数でもtemperatureとtopKという手法を追加した。<br>\n",
    "これにより多様性のある文章が生成される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0.,\\\n",
    "                  layer_eps=1e-05, batch_first = False, T = 10000, N = 1):\n",
    "        super().__init__()\n",
    "        #Tはmax_lenを表している\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,)\n",
    "        self.positional_embedding = nn.Embedding(T, embedding_dim)\n",
    "        self.decoder = nn.ModuleList([PreLNGPTDecoderLayer(embedding_dim, ffn_dim, num_heads, drop_out_rate,\\\n",
    "                                                               layer_eps, batch_first) for _ in range(N)])\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size, bias = False)\n",
    "        self.vocab_size = vocab_size\n",
    "    def forward(self, x, y = None,pad_mask_self = None, mask_self=None):\n",
    "        \"\"\"\n",
    "        yはxを1つだけずらしたデータである\n",
    "        x = data[a:b]なら、y = data[a+1:b+1]となる。\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        pos = torch.arange(0,x.size(1),dtype=torch.long).unsqueeze(0).to(x.device)\n",
    "        pos = self.positional_embedding(pos)\n",
    "        x = x + pos\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, pad_mask_self = pad_mask_self, mask_self = mask_self)\n",
    "        x = self.linear(x)\n",
    "        if y != None:\n",
    "            eps = 0.1\n",
    "            loss = F.cross_entropy(x.view(-1, x.size(-1)), y.view(-1), ignore_index=-1, label_smoothing= eps / (self.vocab_size - 1)) \n",
    "            #ignore_index=-1はyをonehotベクトル化しないでcross_entropyを使うために使用\n",
    "            pred = x.argmax(dim = -1).detach().cpu()\n",
    "            return torch.clamp(loss, min = 1e-32, max = 10e9),pred\n",
    "        loss = None\n",
    "        pred = x[:,[-1],:]\n",
    "        return loss, pred\n",
    "    def create_mask(self, x: torch.tensor, x_pad: int, device: str):\n",
    "        \"\"\"\n",
    "        (batch_size, sequence_length, embedding_dim)の入力を想定\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Trueが無視される値であることに注意すること\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        #srcのマスク制作\n",
    "        padding_mask = (x == x_pad)\n",
    "        mask = torch.triu(torch.ones(size = (seq_len, seq_len))==1).transpose(0,1) #下三角行列を作る\n",
    "        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask==1.,float(0.0)).to(device)\n",
    "        return padding_mask, mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,bos: str, sentence_size, tokenizer, device):\n",
    "        self.eval()\n",
    "        bos_tokenized = tokenizer.encode_ordinary(bos)\n",
    "        bos_tokenized = bos_tokenized[-sentence_size:]\n",
    "        bos_tokenized = torch.LongTensor([bos_tokenized])\n",
    "        _, add_sentence = self(bos_tokenized.to(device))\n",
    "        self.train()\n",
    "        return add_sentence\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_sentence(self, bos: str, sentence_size, generate_tokens, tokenizer, device, top_K = None, temperature = 1.0):\n",
    "        return_sentence = bos\n",
    "        for i in range(generate_tokens):\n",
    "            add_sentence = self.generate(return_sentence, sentence_size, tokenizer,device)\n",
    "            add_sentence = add_sentence[:,-1,:] / temperature #(1, vocab_size)\n",
    "            if top_K is not None:\n",
    "                v, _ = torch.topk(add_sentence, min(top_K, add_sentence.size(-1)))\n",
    "                #v[:, [-1]]がtopkの中でも最小値を取る。これより小さいやつは予想に含めない。\n",
    "                add_sentence[add_sentence < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(add_sentence, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            return_sentence += tokenizer.decode_batch(idx_next.tolist())[0]\n",
    "        return return_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e2ef60554f914b7f3190499c85ea0c48ae4fc01e8f403cc646d16228abbf679"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
