{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このファイルではGPT from scratchで学習したモデルの改善を目標とする。このファイルの立ち位置は同リポジトリに含まれている\n",
    "- pytorch_command.ipynb\n",
    "- attention_from_scratch.ipynb\n",
    "- GPT_from_scratch.ipynb<br>\n",
    "の次に読むことを想定されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA環境が壊れていないことを祈りながら確認->  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "print(\"CUDA環境が壊れていないことを祈りながら確認-> \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreLN\n",
    "GPT from scratchでは、学習中に勾配が変化しない状態がいつまで経っても続く時があった。<br>\n",
    "これはLayer Normalization層がMultiheadAttentionやFFN層の後に置かれるというPostLNの形をとっているからであると最近では言われている<br>\n",
    "このため、改善点の一つとして、サブレイヤーの前にLayer Normalization層を置くPreLNを行う。<br>\n",
    "最近のGPTモデルではPreLNを行なっているようである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLNGPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0., layer_eps=1e-05, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.maskedmultiheadattention = nn.MultiheadAttention(embedding_dim, num_heads,batch_first=batch_first)\n",
    "        self.dropout_selfattn = nn.Dropout(p = drop_out_rate)\n",
    "        self.layernorm_selfattn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, ffn_dim), nn.GELU(), nn.Linear(ffn_dim, embedding_dim))#GELUに変更\n",
    "        self.layernorm_ffn = nn.LayerNorm(embedding_dim, eps = layer_eps)\n",
    "        self.dropout_ffn = nn.Dropout(p = drop_out_rate)\n",
    "\n",
    "    def forward(self, x, pad_mask_self = None, mask_self=None):\n",
    "        #PreLNにする\n",
    "        dx = self.layernorm_selfattn(x)\n",
    "\n",
    "        dx, _ = self.maskedmultiheadattention(dx,dx,dx,key_padding_mask = pad_mask_self, attn_mask = mask_self)\n",
    "\n",
    "        dx = self.dropout_selfattn(dx)\n",
    "\n",
    "        x = x+dx\n",
    "\n",
    "        dx = self.layernorm_ffn(x)\n",
    "\n",
    "        dx = self.dropout_ffn(self.ffn(dx))\n",
    "\n",
    "        x = x + dx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改善したGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このPreLNGPTDecoderLayerを用いてGPTモデルを制作する。ところで、今回はGPTのgenerate_sentence関数でもtemperatureとtopKという手法を追加した。<br>\n",
    "これにより多様性のある文章が生成される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, ffn_dim, num_heads, drop_out_rate = 0.,\\\n",
    "                  layer_eps=1e-05, batch_first = False, T = 10000, N = 1):\n",
    "        super().__init__()\n",
    "        #Tはmax_lenを表している\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,)\n",
    "        self.positional_embedding = nn.Embedding(T, embedding_dim)\n",
    "        self.decoder = nn.ModuleList([PreLNGPTDecoderLayer(embedding_dim, ffn_dim, num_heads, drop_out_rate,\\\n",
    "                                                               layer_eps, batch_first) for _ in range(N)])\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size, bias = False)\n",
    "        self.vocab_size = vocab_size\n",
    "    def forward(self, x, y = None,pad_mask_self = None, mask_self=None):\n",
    "        \"\"\"\n",
    "        yはxを1つだけずらしたデータである\n",
    "        x = data[a:b]なら、y = data[a+1:b+1]となる。\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        pos = torch.arange(0,x.size(1),dtype=torch.long).unsqueeze(0).to(x.device)\n",
    "        pos = self.positional_embedding(pos)\n",
    "        x = x + pos\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, pad_mask_self = pad_mask_self, mask_self = mask_self)\n",
    "        x = self.linear(x)\n",
    "        if y != None:\n",
    "            loss = F.cross_entropy(x.view(-1, x.size(-1)), y.view(-1), ignore_index=-1) \n",
    "            #ignore_index=-1はyをonehotベクトル化しないでcross_entropyを使うために使用\n",
    "            pred = x.argmax(dim = -1).detach().cpu()\n",
    "            return loss,pred\n",
    "        loss = None\n",
    "        pred = x[:,[-1],:]\n",
    "        return loss, pred\n",
    "    def create_mask(self, x: torch.tensor, x_pad: int, device: str):\n",
    "        \"\"\"\n",
    "        (batch_size, sequence_length, embedding_dim)の入力を想定\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Trueが無視される値であることに注意すること\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        #srcのマスク制作\n",
    "        padding_mask = (x == x_pad)\n",
    "        mask = torch.triu(torch.ones(size = (seq_len, seq_len))==1).transpose(0,1) #下三角行列を作る\n",
    "        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask==1.,float(0.0)).to(device)\n",
    "        return padding_mask, mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,bos: str, sentence_size, tokenizer, device):\n",
    "        self.eval()\n",
    "        bos_tokenized = tokenizer.encode_ordinary(bos)\n",
    "        bos_tokenized = bos_tokenized[-sentence_size:]\n",
    "        bos_tokenized = torch.LongTensor([bos_tokenized])\n",
    "        _, add_sentence = self(bos_tokenized.to(device))\n",
    "        self.train()\n",
    "        return add_sentence\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_sentence(self, bos: str, sentence_size, generate_tokens, tokenizer, device, top_K = None, temperature = 1.0):\n",
    "        return_sentence = bos\n",
    "        for i in range(generate_tokens):\n",
    "            add_sentence = self.generate(return_sentence, sentence_size, tokenizer,device)\n",
    "            add_sentence = add_sentence[:,-1,:] / temperature #(1, vocab_size)\n",
    "            if top_K is not None:\n",
    "                v, _ = torch.topk(add_sentence, min(top_K, add_sentence.size(-1)))\n",
    "                #v[:, [-1]]がtopkの中でも最小値を取る。これより小さいやつは予想に含めない。\n",
    "                add_sentence[add_sentence < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(add_sentence, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            return_sentence += tokenizer.decode_batch(idx_next.tolist())[0]\n",
    "        return return_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap(\"bin/train.bin\", dtype = np.uint16, mode = \"r\")\n",
    "val_data = np.memmap(\"bin/val.bin\", dtype = np.uint16, mode = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size = 1024\n",
    "batch_size = 5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def get_batch(split: str, batch_size = 256,device = \"cpu\")->torch.Tensor:\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - sentence_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+sentence_size]).astype(np.int64)) for i in index])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+sentence_size]).astype(np.int64)) for i in index])\n",
    "    if device == \"cuda\":\n",
    "        return x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_size = 768\n",
    "num_heads = 6\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "#KarpathyのminGPTを参考に、パラメーターを設定した。\n",
    "depth = 6\n",
    "gpt = GPT(50257, embedding_size, embedding_size*4, num_heads, 0, batch_first=True, T = sentence_size, N = depth).to(device) \n",
    "#事前学習のときはDropout無し、ファインチューニングのときはありが好ましい\n",
    "warmup_iters = 2000\n",
    "\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max = 2.5e-4\n",
    "min_lr = 2.5e-5\n",
    "max_iters = 10000\n",
    "def get_lr(cur_iter):\n",
    "    #cur_iter現在のiteration\n",
    "    if cur_iter < warmup_iters:\n",
    "        return lr_max * cur_iter / warmup_iters\n",
    "    return max(lr_max * (np.cos(cur_iter / max_iters * np.pi) + 1), min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'最初の訓練時のコードでは以下のように初期化を行なう。\\nimport gc\\nfrom tqdm import tqdm\\nbatch_iteration = 128\\nscaler = torch.cuda.amp.GradScaler(enabled=True)\\nbest_loss = 1e9\\nbegin = 0\\nval_iteration = 1\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"最初の訓練時のコードでは以下のように初期化を行なう。\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "batch_iteration = 128\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "best_loss = 1e9\n",
    "begin = 0\n",
    "val_iteration = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 289/6805 [3:16:17<73:35:51, 40.66s/it]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "checkpoint = torch.load(\"latest_checkpoint.bin\") #チェックポイントがあるなら使う\n",
    "batch_iteration = 128\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True) #defaultだとinitが大きすぎるので治す\n",
    "best_loss = checkpoint[\"best_loss\"]\n",
    "begin = checkpoint[\"iter\"]\n",
    "val_iteration = 1\n",
    "gpt.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "del checkpoint\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for cur_iter in tqdm(range(begin,max_iters)):\n",
    "    optimizer.lr = get_lr(cur_iter+1)\n",
    "    for batch_iter in range(batch_iteration):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            x,y = get_batch(\"train\",batch_size=batch_size,device=device)\n",
    "            padding_mask, mask = gpt.create_mask(x, 0, device)\n",
    "            loss, pred = gpt(x,y,padding_mask,mask)\n",
    "        scaler.scale(loss).backward() \n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "        del x, y\n",
    "        del padding_mask, mask\n",
    "        del loss\n",
    "        del pred\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    for val_iter in range(val_iteration):\n",
    "        with torch.no_grad(): #こうしないとCUDAERRORが起きる\n",
    "            with torch.amp.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                x,y = get_batch(\"valid\",batch_size=batch_size,device=device)\n",
    "                padding_mask, mask = gpt.create_mask(x, 0, device)\n",
    "                loss, pred = gpt(x,y,padding_mask,mask)\n",
    "        if best_loss > loss.detach().item():\n",
    "            best_loss = loss.detach().item()\n",
    "            checkpoint = {\n",
    "                \"model\": gpt.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scaler\": scaler,\n",
    "                \"iter\": cur_iter,\n",
    "                \"best_loss\": best_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, \"best_checkpoint.bin\")\n",
    "            print(\"params updated. BestLoss: \", best_loss)\n",
    "            with open(\"learning_detail.txt\",\"w\") as f:\n",
    "                f.write(\"学習状況\\n\")\n",
    "                f.write(f\"iter: {cur_iter}\\n\")\n",
    "                f.write(f\"hyper params: \\n\")\n",
    "                f.write(f\"vocab_size: 50257, embedding size: {embedding_size}, ffn: {embedding_size*4}, num_heads: {num_heads}, Depth: {depth}, sentnce_size: {sentence_size}\\n\")\n",
    "                f.write(f\"lr: {optimizer.lr},best_loss: {best_loss}\\n\")\n",
    "                f.close()\n",
    "        del x, y\n",
    "        del padding_mask, mask\n",
    "        del pred\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    if torch.isnan(loss.detach()):\n",
    "        print(\"Loss is NaN!\")\n",
    "        break\n",
    "    checkpoint = {\n",
    "    \"model\": gpt.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"scaler\": scaler,\n",
    "    \"iter\": cur_iter,\n",
    "    \"best_loss\": best_loss,\n",
    "    \"loss\": loss\n",
    "    }\n",
    "    torch.save(checkpoint, \"latest_checkpoint.bin\")\n",
    "    with open(\"learning_detail_latest.txt\",\"w\") as f:\n",
    "        f.write(\"学習状況\\n\")\n",
    "        f.write(f\"iter: {cur_iter}\\n\")\n",
    "        f.write(f\"hyper params: \\n\")\n",
    "        f.write(f\"vocab_size: 50257, embedding size: {embedding_size}, ffn: {embedding_size*4}, num_heads: {num_heads}, Depth: {depth}, sentnce_size: {sentence_size}\\n\")\n",
    "        f.write(f\"lr: {optimizer.lr},best_loss: {best_loss}\\n\")\n",
    "        f.write(f\"val_loss: {loss}\\n\")\n",
    "        f.close()\n",
    "        del loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e2ef60554f914b7f3190499c85ea0c48ae4fc01e8f403cc646d16228abbf679"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
